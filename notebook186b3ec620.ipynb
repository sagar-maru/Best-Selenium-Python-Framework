{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"provenance":[],"toc_visible":true},"gpuClass":"standard","kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Gesture Recognition Project","metadata":{"id":"8FG0dc1jEnFA"}},{"cell_type":"markdown","source":"## Problem Statement\n\nAs a data scientist at a home electronics company specializing in cutting-edge smart televisions, my objective is to develop an innovative feature that enables users to control their TV through gesture recognition. This feature will be powered by a webcam mounted on the TV, continuously monitoring and interpreting five distinct gestures, allowing users to interact with the device without needing a remote control.\n\nThe five gestures and their corresponding actions are as follows:\n\n1. **Thumbs Up**: This gesture will be recognized to increase the volume of the TV. When the user gives a thumbs up, the system will detect the gesture and adjust the volume settings accordingly.\n\n2. **Thumbs Down**: Similarly, the thumbs down gesture will decrease the volume when detected, providing a hands-free way to control sound levels.\n\n3. **Left Swipe**: A leftward swipe will trigger the \"Jump\" command, rewinding the video content by 10 seconds. This gesture allows users to easily revisit a recent moment in their video.\n\n4. **Right Swipe**: In contrast to the left swipe, a right swipe will \"Jump\" forward by 10 seconds, allowing users to skip ahead without having to manually interact with the interface.\n\n5. **Stop**: A stop gesture will pause the content being played. This gesture provides a quick and intuitive way to halt the video, offering users more control during their viewing experience.\n\nTo implement this feature, machine learning algorithms will be employed to train the system to accurately recognize these gestures in real-time. The webcam will continuously capture the user’s movements, and the model will process this input to execute the corresponding commands. This gesture-based control system aims to enhance user convenience and redefine the smart TV experience by providing a seamless, hands-free interaction method.\n\nEach video is a sequence of 30 frames (or images)","metadata":{"id":"m_HsK5jJjVfs"}},{"cell_type":"markdown","source":"## Objectives\nThe objective is to develop a gesture-based control feature for smart TVs, enabling users to manage volume, skip, rewind, and pause content using five specific gestures detected by a webcam. Machine learning algorithms will train the system to recognize gestures in real-time, offering a hands-free, intuitive TV experience.\n\n1. **Understanding the Dataset**: The training dataset consists of several hundred videos, each categorized into one of five gesture classes. Each video typically lasts 2-3 seconds and is divided into a sequence of 30 frames (images). These videos were captured by different individuals performing one of the five predefined gestures in front of a webcam, similar to the setup used in the smart TV application. The gestures include actions like thumbs up, thumbs down, left swipe, right swipe, and stop, and each video serves as a unique sample for training the gesture recognition model.\n\n2. **Generator**: The generator is responsible for preprocessing the video data and feeding it into the model. It should be capable of processing batches of videos without errors, including essential steps such as cropping, resizing, and normalization. These steps ensure that the input data is properly formatted and standardized before being passed to the model for training.\n\n3. **Model**: The goal is to develop a model that trains efficiently and without errors, while keeping the inference (prediction) time as low as possible. This will be determined by the total number of parameters in the model—fewer parameters lead to faster inference. The model’s performance will also be evaluated based on its accuracy in correctly recognizing the gestures. As suggested by Snehansu, the model should first be trained on a small dataset to evaluate its initial performance before scaling up to larger datasets.\n\n4. **Write-up**: The write-up should document the entire process of model selection and development. It will begin with the rationale for choosing the base model, followed by a detailed discussion of the modifications and experiments conducted to improve performance. Key metrics, such as accuracy and inference time, should be considered when refining the model, with a focus on optimizing the trade-off between performance and efficiency to arrive at the final, most effective solution.","metadata":{"id":"MN0YqnPajgjc"}},{"cell_type":"code","source":"## Checking the GPU configuration\n!nvidia-smi","metadata":{"id":"yPy4J153ybm6","outputId":"bdc6f18d-d2ca-4499-8f52-36bbfa4d1331"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Import Required Libraries\nThis code imports essential libraries and modules for deep learning and image processing tasks. **`numpy`** is used for numerical operations and array manipulations. **`os`** allows interaction with the operating system for file and directory management. **`imageio`** and **`PIL`** are used for image reading and resizing. **`matplotlib.pyplot`** is for plotting visualizations. **Keras and TensorFlow** provide tools for building and training deep learning models, including layers like **`Dense`**, **`LSTM`**, and **`Conv3D`**, as well as optimization and model callbacks. Pre-trained models like **`VGG16`** and **`MobileNet`** are imported for transfer learning tasks.","metadata":{"id":"LutTwYGpIrFt"}},{"cell_type":"code","source":"import numpy as np\nimport os\nimport imageio # To imread and imresize\nimport datetime\nfrom PIL import Image # To imread and imresize\nimport matplotlib.pyplot as plt\n\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, LSTM, GRU, Flatten, TimeDistributed, Conv2D, BatchNormalization, ConvLSTM2D, Activation, Dropout, MaxPooling2D, Dense, GlobalAveragePooling3D, GlobalAveragePooling2D\nfrom keras.layers.convolutional import Conv3D, MaxPooling3D, MaxPooling2D\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras import optimizers, regularizers\n\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications import mobilenet","metadata":{"id":"FTPuEhscEDL4"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This code sets the random seed to ensure reproducibility of results across different runs of the model. **`np.random.seed(30)`** fixes the random seed for NumPy, ensuring consistent results in numerical operations. **`random.seed(30)`** sets the random seed for Python's built-in random module. **`K.backend`** from Keras ensures consistency in the backend computations, particularly in deep learning models. Finally, **`tf.random.set_seed(30)`** fixes the seed for TensorFlow's random number generation, which is essential for reproducibility in model initialization, data shuffling, and other operations involving randomness. This step helps ensure stable results across experiments.","metadata":{"id":"WKpf81npEJ9x"}},{"cell_type":"code","source":"# We set the random seed so that the results don't vary drastically.\nnp.random.seed(30)\nimport random as rn\nrn.seed(30)\nfrom keras import backend as K\nimport tensorflow as tf\ntf.random.set_seed(30)","metadata":{"id":"7LiVSTg7ElPU"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Reading the Structured Data\nReading structured data involves loading and processing data stored in organized formats, such as CSV, Excel, or SQL databases, into a format that can be easily manipulated and analyzed. In Python, the `pandas` library is commonly used for this task, as it provides powerful data structures like DataFrames for handling tabular data. Once loaded, you can perform operations such as filtering, sorting, aggregating, and transforming the data. Structured data is typically used in machine learning, data analysis, and data visualization tasks. The process ensures that the data is properly formatted for further analysis or model training.","metadata":{"id":"Dg_tJ8YtEJ9y"}},{"cell_type":"markdown","source":"## Upload the Datasets in Colab\nTo execute the code in Google Colab (https://colab.research.google.com/), use the following to upload a dataset","metadata":{"id":"p8OJROSMNy4d"}},{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"id":"ClBLBFaSNjK9"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Set the path of the file\n# path_to_dataset = \"/content/drive/MyDrive/Gesture_Recognition/Project_data.zip\"\n# path = \"/content/drive/MyDrive/Gesture_Recognition/Project_data\"","metadata":{"id":"AWjI_YbYM8mb"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ! ls /content/drive/MyDrive/Gesture_Recognition","metadata":{"id":"1ilq3VLCS5zK"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Unzip the dataset from google drive in local environment in the folder dataset\n# import zipfile\n# zip_ref = zipfile.ZipFile(path_to_dataset, 'r')\n# zip_ref.extractall(\"/dataset\")\n# zip_ref.close()","metadata":{"id":"ZsvYi3OtRqeC"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Read the Folder Names for Training and Validation.\n\n**Note:** Set the batch size in such a way that we can use the GPU in full capacity. We keep increasing the batch size until the machine throws an error.\n\nThe code snippet demonstrates how to read and randomly shuffle structured data from CSV files. The `folder_name` specifies the directory containing the dataset files. Using `np.random.permutation()`, the rows of the `train.csv` and `val.csv` files are read and shuffled to introduce randomness into the dataset, which helps avoid bias during training. The `batch_size` variable defines the number of samples that will be processed together in one iteration during model training. This approach ensures that the data is well-prepared for training and validation, promoting better model generalization and avoiding overfitting.","metadata":{"id":"Tv9VjGhfImiF"}},{"cell_type":"code","source":"folder_name = '/home/datasets/Project_data'\ntrain_doc = np.random.permutation(open(folder_name + '/train.csv').readlines())\nval_doc = np.random.permutation(open(folder_name + '/val.csv').readlines())\nbatch_size = 64  # experiment with the batch size\n\n# Set the training and validation data set path\ntrain_path = '/datasets/Project_data/train'\nval_path = '/datasets/Project_data/val'","metadata":{"id":"fJvof9UHIIZk"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Developing Reusable Functions\nCreating generic functions involves developing reusable and modular code that can perform specific tasks across different scenarios. These functions are designed to be flexible and adaptable to various inputs, which allows them to be used multiple times without modification. This approach promotes code reusability, reduces redundancy, and enhances maintainability, making your codebase cleaner and more efficient. By writing generic functions, you can streamline the development process and improve collaboration in team projects.","metadata":{"id":"prnMTIdoEJ90"}},{"cell_type":"markdown","source":"## Create Callback Directory Function\nThe `callback_directory()` function creates a directory to save model checkpoints and metrics during training. It generates a unique model directory using the current date and time. If the directory doesn't exist, it creates it. The function sets up a **ModelCheckpoint** callback to save the model at each epoch, including training and validation accuracy, loss, and other metrics in the filename. Additionally, the **ReduceLROnPlateau** callback reduces the learning rate when the validation loss stops improving. These callbacks are returned in a list to be used during model training, allowing for the saving and monitoring of model performance over epochs.","metadata":{"id":"Pz6JS-jbkRgs"}},{"cell_type":"code","source":"# Let us save each model epochs in a directory by making check points and also save the model metrics (training accuracy, traing loss ,validation loss, validation accuracy)\ndef callback_directory(curr_dt_time):\n  model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n\n  if not os.path.exists(model_name):\n    os.mkdir(model_name)\n\n  filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.keras'\n\n  checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='auto', save_freq='epoch')\n\n  LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, mode='min', min_delta=0.0001, cooldown=0, min_lr=0.00001)\n  callbacks_list = [checkpoint, LR]\n  return callbacks_list","metadata":{"id":"_qBGrq91kQVW"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Create Normalise Function\n\nThe `normalise` function is a simple utility designed to scale the pixel values of an image to the range [0, 1]. This is typically done to standardize the input data for neural networks, improving model training stability and performance.\n\n#### How it Works:\n- **Input**: The function takes an image represented as a NumPy array, where pixel values typically range from 0 to 255 (standard 8-bit color channels).\n- **Process**: It divides each pixel value by 255, which normalizes the values to the range [0, 1].\n- **Output**: The output is the normalized image, where each pixel value is now between 0 and 1, making it more suitable for training deep learning models.\n\n#### Example:\n```python\nimport numpy as np\n\n# Sample image with pixel values ranging from 0 to 255\nimage = np.array([[0, 128, 255], [64, 192, 255]])\n\n# Normalize the image\nnormalized_image = normalise(image)\n\nprint(normalized_image)\n```\n\nOutput:\n```python\n[[0.         0.50196078 1.        ]\n [0.25098039 0.75294118 1.        ]]\n```\n\n#### Why Normalize?\n- **Improves Convergence**: Neural networks often perform better when input data is normalized. This ensures the model doesn't get biased by large values, speeding up convergence during training.\n- **Consistency**: By scaling the values to a consistent range, the model can learn features in a more consistent manner, leading to better generalization.\n\n","metadata":{"id":"78YA04wAEJ91"}},{"cell_type":"code","source":"def normalise(image):\n  return image/255.","metadata":{"id":"ywvCXIYJEJ91"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Customized Generator Function\n\n### Explanation of the `generator` Function\n\nThe `generator` function is designed to load and preprocess batches of video frames for training deep learning models, specifically those working with 3D convolutional networks (Conv3D). This function ensures that data is loaded efficiently in batches and can be used for training without requiring the entire dataset to fit into memory at once. Here's a detailed explanation of how it works:\n\n#### Parameters:\n- **`source_path`**: The directory where the video frames are stored.\n- **`folder_list`**: A list containing the paths to the folders that store the video frames.\n- **`batch_size`**: The number of samples (videos) to be processed in each batch.\n- **`y` and `z`**: The desired dimensions (height, width) to which each image will be resized.\n- **`normalise`**: A function used to normalize the image pixel values (e.g., scaling pixel values to a range between 0 and 1).\n\n#### Function Workflow:\n1. **Initialize Image Index and Parameters**:\n   - The function begins by setting the image indices (`img_idx`) to select specific frames from each video. These indices will define which frames are used for the batch.\n   - The variables `x`, `y`, and `z` define the number of images per video (from `img_idx`) and the target image dimensions.\n\n2. **Data Shuffling**:\n   - `np.random.permutation(folder_list)` shuffles the list of folders (i.e., videos) to ensure that the model sees a diverse set of samples during training, which helps prevent overfitting.\n\n3. **Batch Preparation**:\n   - The number of batches is calculated by dividing the total number of samples by the `batch_size`.\n   - For each batch, the function initializes two arrays:\n     - **`batch_data`**: This will hold the image data, with the shape `(batch_size, x, y, z, 3)`, where 3 corresponds to the RGB color channels.\n     - **`batch_labels`**: This will store one-hot encoded labels, with the shape `(batch_size, 5)`, assuming 5 classes for classification.\n\n4. **Reading and Processing Images**:\n   - For each video folder in the batch, the images are loaded sequentially. The function reads the image file paths and processes them:\n     - If the image width is `160`, it crops the image to `120x120` to maintain consistency in input size.\n     - The image is resized to the target size `(y, z)` and converted to a NumPy array.\n     - The image is normalized using the provided `normalise` function for each color channel (R, G, B).\n\n5. **One-Hot Encoding of Labels**:\n   - Each video folder has a corresponding label (represented by an index). The label is one-hot encoded and placed in the `batch_labels` array.\n\n6. **Yielding Batches**:\n   - After processing all the images in the batch, the function yields the `batch_data` (input images) and `batch_labels` (one-hot encoded labels) to be used by the model during training.\n\n7. **Handling Remaining Data**:\n   - If the dataset doesn't divide evenly into batches (i.e., if there are leftover samples), the function processes the remaining samples in a smaller batch. It ensures that these leftover samples are also processed correctly by the generator.\n\n8. **Infinite Loop**:\n   - The function runs in an infinite loop (`while True`), meaning that it will continue generating batches indefinitely, allowing the training process to continue until manually stopped or until a specified number of epochs is reached.\n\n#### Benefits:\n- **Memory Efficiency**: The generator only loads and processes one batch of images at a time, reducing memory usage and enabling the training of models on large datasets.\n- **Randomization**: The data is shuffled before each batch, improving the model’s generalization by preventing the model from memorizing the order of the data.\n- **On-the-Fly Augmentation**: The function resizes and normalizes images, ensuring the input data is in a consistent format, suitable for training deep learning models.\n\n### Example Usage:\nThis function would typically be used in the `.fit()` method of a Keras model:\n```python\nmodel.fit(generator(source_path, folder_list, batch_size, y, z, normalise), epochs=10, steps_per_epoch=len(folder_list)//batch_size)\n```\n\nThis allows the model to train using batches generated on-the-fly, ensuring efficient memory usage and data augmentation throughout the training process.","metadata":{"id":"2vaWs_-q-3r-"}},{"cell_type":"code","source":"def generator(source_path, folder_list, batch_size,y,z,normalise):\n    print( 'Source path = ', source_path, '; batch size =', batch_size,';Image resolution = ({},{})'.format(y,z))\n    img_idx = [0,1,2,4,6,8,10,12,14,16,18,20,22,24,26,27,28,29] # list(range(0, 32, 2)) #create a list of image numbers you want to use for a particular video\n    # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n    x=len(img_idx)\n    while True:\n        t = np.random.permutation(folder_list)\n        num_batches =  int(len(t)/batch_size)  # calculate the number of batches\n        for batch in range(num_batches): # we iterate over the number of batches\n            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n            for folder in range(batch_size): # iterate over the batch_size\n                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n                    image = Image.open(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n\n                    #crop the images and resize them. Note that the images are of 2 different shape\n                    #and the conv3D will throw error if the inputs in a batch have different shapes\n\n                    if(image.size[0] == 160): # crop the image of shape 120 X 160 to make it  120 X120\n                      image = image.crop((20,0,140,120)) # left =20, top =0, right = 140, bottom = 120\n\n                    image = image.resize(size = (y,z)) #imageio.imresize(image,(y,z)).astype(np.float32)\n                    image = np.asarray(image).astype(np.float32)\n\n                    batch_data[folder,idx,:,:,0] = normalise(image[:,:,0])  #normalise and feed in the image\n                    batch_data[folder,idx,:,:,1] = normalise(image[:,:,1]) #normalise and feed in the image\n                    batch_data[folder,idx,:,:,2] = normalise(image[:,:,2]) #normalise and feed in the image\n\n                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n\n\n        # write the code for the remaining data points which are left after full batches\n\n        if (len(t) % batch_size) != 0:\n            batch_data = np.zeros((len(t)%batch_size,x,y,z,3))\n            batch_labels = np.zeros((len(t)%batch_size,5))\n            for folder in range(len(t)%batch_size):\n                imgs = os.listdir(source_path+'/'+ t[folder + (num_batches*batch_size)].split(';')[0])\n                for idx,item in enumerate(img_idx):\n                    image = Image.open(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n                    if image.size[0] == 160:\n                        image = image.crop((20,0,140,120)) # left =20, top =0, right = 140, bottom = 120\n\n                    image = image.resize(size = (y,z))\n                    image = np.asarray(image).astype(np.float32)\n\n\n                    batch_data[folder,idx,:,:,0] = normalise(image[:,:,0])  #normalise and feed in the image\n                    batch_data[folder,idx,:,:,1] = normalise(image[:,:,1]) #normalise and feed in the image\n                    batch_data[folder,idx,:,:,2] = normalise(image[:,:,2]) #normalise and feed in the image\n\n                batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1\n\n            yield batch_data, batch_labels","metadata":{"id":"gBWiyfAyyW2J"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Print Model Metrics\n\n- **Hyper String**: The `hyper_string` is created by repeating the word \"hyper\" 50 times using the multiplication operator `\"hyper \" * 50`.\n- **Print Hyper**: The string is then printed before displaying the training and validation metrics.\n\n### Example Usage:\n```python\n# Assuming 'model10' is the model name and 'history_model10' is the history object\nprint_model_metrics_with_hyper(\"cnn_gru_model\", history_model10)\n```\n\nThis will print the model name followed by the word \"hyper\" repeated 50 times, then the loss and accuracy metrics for both training and validation.","metadata":{"id":"_2Ej62fNu5Le"}},{"cell_type":"code","source":"def print_model_metrics(model_name, history):\n    # Print the model name\n    print(\"-\" * 50)\n    print(f\"Model: {model_name}\")\n    print(\"-\" * 50)\n\n    # Print the training loss values over all epochs\n    print(\"Training Loss: \", history.history['loss'])\n\n    # Print the training categorical accuracy values over all epochs\n    print(\"Training Categorical Accuracy: \", history.history['categorical_accuracy'])\n\n    # Print the validation loss values over all epochs\n    print(\"Validation Loss: \", history.history['val_loss'])\n\n    # Print the validation categorical accuracy values over all epochs\n    print(\"Validation Categorical Accuracy: \", history.history['val_categorical_accuracy'])","metadata":{"id":"yCIqiobPu5Le"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Plot Training and Validation Accuracy and Loss.\nThis code defines a function plot_history() to plot training and validation accuracy and loss over epochs. It extracts accuracy and loss values from the training history and plots them using matplotlib. The function generates two subplots: one for accuracy and one for loss, showing the progress of both training and validation metrics.","metadata":{"id":"A_3GsLqqj-OE"}},{"cell_type":"code","source":"# Plot the graph for accuracy and loss of both training and validation dataset\ndef plot_history(history, num_epochs):\n    # Extracting accuracy and loss from history\n    acc, val_acc = history.history['categorical_accuracy'], history.history['val_categorical_accuracy']\n    loss, val_loss = history.history['loss'], history.history['val_loss']\n\n    epochs_range = range(num_epochs)\n\n    # Create subplots for accuracy and loss\n    fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n\n    # Plotting accuracy\n    axes[0].plot(epochs_range, acc, label='Training Accuracy')\n    axes[0].plot(epochs_range, val_acc, label='Validation Accuracy')\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Accuracy\")\n    axes[0].set_title('Training and Validation Accuracy')\n    axes[0].legend(loc='lower right')\n\n    # Plotting loss\n    axes[1].plot(epochs_range, loss, label='Training Loss')\n    axes[1].plot(epochs_range, val_loss, label='Validation Loss')\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Loss\")\n    axes[1].set_title('Training and Validation Loss')\n    axes[1].legend(loc='upper right')\n\n    # Display the maximum accuracy values\n    print(\"Max. Training Accuracy:\", max(history.history['categorical_accuracy']))\n    print(\"Max. Validation Accuracy:\", max(history.history['val_categorical_accuracy']))\n\n    # Display the plots\n    plt.tight_layout()\n    plt.show()","metadata":{"id":"jffyQxFRj9kf"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualizing Data\nVisualizing Data involves using graphical representations to better understand the structure, patterns, and relationships within the dataset. It plays a crucial role in data exploration and analysis by helping identify trends, detect outliers, and communicate findings effectively. In machine learning and deep learning, visualizations help in understanding the distribution of input data, evaluating model performance, and debugging model behavior.","metadata":{"id":"Ycfkw2SeEJ91"}},{"cell_type":"markdown","source":"**Read the sample image and explore its properties through experimentation.**\n\n1. **`os.listdir()`**: Lists all the files in the directory specified by `folder_name + '/train/' + train_doc[0].split(';')[0]`. This directory corresponds to a sequence of images (video frames) from the dataset.\n   \n2. **`train_doc[0].split(';')[0]`**: Extracts the folder name from the first entry in the `train_doc` list by splitting it at the semicolon and selecting the first part (the folder path).\n\n3. **`imageio.imread()`**: Reads the first image from the selected folder using the `imageio` library, which handles various image formats. The first image file is selected from the list of images `imgs`.\n\n4. **`astype(np.float32)`**: Converts the image to the `float32` data type, which is commonly used in deep learning tasks for more efficient computation and higher precision.\n\nThe result is a sample image loaded into memory, ready for preprocessing or model input.","metadata":{"id":"66dPZ5b1WP_q"}},{"cell_type":"code","source":"imgs = os.listdir(folder_name+'/train/'+ train_doc[9].split(';')[0]) # read a sample sequence of images (a video) from the folder\nimage = imageio.imread(folder_name+'/train/'+ train_doc[9].strip().split(';')[0]+'/'+imgs[0]).astype(np.float32)\nprint(image)\nprint('-'*50)\nprint(imgs)","metadata":{"id":"PMw3gmPyfQAm","outputId":"7e2abbc1-c05c-4efb-a1ba-f6ed89c7582c"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Display images and resize images\n1. **Function Definition**: The function `display_and_resize_images` is created to encapsulate the image loading, displaying, and resizing logic.\n   \n2. **Path Construction**: The `folder_path` variable constructs the path to the folder containing images. This ensures that the file paths are built correctly, regardless of the operating system.\n\n3. **Original Images Display**:\n   - `plt.figure(figsize=(8, 8))`: Sets up the figure size.\n   - The loop iterates over each image file in the folder (`imgs`) and displays it using `plt.imshow(im)`.\n   - Axis ticks and grid are hidden to focus on the images themselves.\n\n4. **Image Resize**:\n   - After displaying the original images, the code resizes each image to `100x100` and displays them again.\n   - `im.resize((100, 100))`: Resizes the image to the specified size.\n\n5. **Print Statements**: After displaying both the original and resized images, the shape of the last image is printed to show the dimensions.\n\nThis version of the code is cleaner, more modular, and easier to understand. It separates the logic into a function and uses `os.path.join()` for cross-platform compatibility.","metadata":{"id":"RJtgAWPyEJ92"}},{"cell_type":"code","source":"def display_and_resize_images(folder_name, train_doc):\n    # Extract the folder containing images from the train_doc\n    folder_path = os.path.join(folder_name, 'train', train_doc[9].split(';')[0])\n\n    # List all the images (frames) in the folder\n    imgs = os.listdir(folder_path)\n\n    # Display original images\n    plt.figure(figsize=(8, 8))\n    for item in range(len(imgs)):  # Iterate over the frames/images\n        img_path = os.path.join(folder_path, imgs[item])\n        im = Image.open(img_path)\n        im = np.asarray(im)\n\n        plt.subplot(6, 5, item + 1)\n        plt.xticks([])  # Hide x-axis ticks\n        plt.yticks([])  # Hide y-axis ticks\n        plt.grid(False)  # Disable grid\n        plt.imshow(im)  # Show the image\n\n    plt.show()\n    print(f\"Original image shape: {im.shape}\")\n\n    # Resize and display images\n    print('***********************After Resize*******************************************')\n    plt.figure(figsize=(8, 8))\n    for item in range(len(imgs)):  # Iterate over the frames/images\n        img_path = os.path.join(folder_path, imgs[item])\n        im = Image.open(img_path)\n        im = im.resize((100, 100))  # Resize the image to 100x100\n        im = np.asarray(im)\n\n        plt.subplot(6, 5, item + 1)\n        plt.xticks([])  # Hide x-axis ticks\n        plt.yticks([])  # Hide y-axis ticks\n        plt.grid(False)  # Disable grid\n        plt.imshow(im)  # Show the resized image\n\n    plt.show()\n    print(f\"Resized image shape: {im.shape}\")\n\ndisplay_and_resize_images(folder_name, train_doc)","metadata":{"id":"WCXGs82tZAeL","outputId":"38cf63fa-b692-4cef-ce3c-d3954c58fc80"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a figure and axis for plotting 3 images horizontally\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Loop through the images and plot them\nfor i in range(3):\n    # Construct the path for each image\n    image_path = os.path.join(folder_name, 'train', train_doc[9].split(';')[0], imgs[i])\n\n    # Open the image using Image.open\n    im = Image.open(image_path)\n\n    # Display the image on the corresponding subplot\n    axes[i].imshow(im)\n    axes[i].axis('off')  # Hide axes for a cleaner display\n\n    # Get and print the size of the image\n    image_size = im.size\n    print(f\"Image {i+1} size: {image_size}\")\n\n# Show the plot\nplt.show()","metadata":{"id":"nLUt30uNC8ib","outputId":"0bdb5f21-14fe-46bc-833e-18c710392746"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a figure and axis for plotting 3 images horizontally\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Loop through the images, crop them, and display them\nfor i in range(3):\n    # Construct the path for each image\n    image_path = os.path.join(folder_name, 'train', train_doc[9].split(';')[0], imgs[i])\n\n    # Open the image using Image.open\n    im = Image.open(image_path)\n\n    # Get the current width and height of the image\n    width, height = im.size\n\n    # Define the coordinates for cropping (left, top, right, bottom)\n    left = 20\n    top = 0\n    right = width - 20\n    bottom = height\n\n    # Crop the image and convert it to a NumPy array\n    im_cropped = np.array(im.crop((left, top, right, bottom)))\n\n    # Display the cropped image on the corresponding subplot\n    axes[i].imshow(im_cropped)\n    axes[i].axis('off')  # Hide axes for a cleaner display\n\n    # Check and print the shape of the image after cropping\n    print(f\"Image {i+1} shape after cropping: {im_cropped.shape}\")\n\n# Show the plot\nplt.show()","metadata":{"id":"vylfrVtx_IaO","outputId":"2cb0ba31-ee2c-479f-95e8-58a9bad8dd00"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Note:** The dataset has 2 different size images (360 X 360 ) and (160 X 120)","metadata":{"id":"Ar0MW5U6un6I"}},{"cell_type":"markdown","source":"The code creates a NumPy array `arr` with shape `(3, 3)` representing pixel values of an image. Each element in the array corresponds to a pixel intensity value, where the values range from 0 to 255, typically representing RGB color channels.\n\nBy dividing the array `arr` by 255 (`arr/255`), the pixel values are normalized to a range between 0 and 1. This is a common preprocessing step in image processing, especially for machine learning models, as it scales the pixel values to a range that makes the training process more stable and efficient. After this operation, the values of the array become fractions of the original range, suitable for input into neural networks or other algorithms that require normalized data.\n\nFor example:\n- `120 / 255` will become approximately `0.47`\n- `130 / 255` will become approximately `0.51`\n- `255 / 255` will become `1.0`\n\nThis normalization process ensures that the input data has consistent scaling, making it easier for models to learn.","metadata":{"id":"vpYDrKy5Fghz"}},{"cell_type":"code","source":"arr = np.array([[120,130,255],[250,255,0],[166,255,1]])\narr/255.","metadata":{"id":"81CVchcSGA4-","outputId":"397308d8-71f8-43ba-e27b-f0b03fe873f0"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Conv3D Model Architecture\n\nIn this section, we design a deep learning model using Keras, leveraging several powerful functionalities to address the specific requirements of the task. The model is built to handle video or time-series data and is designed for efficiency in terms of accuracy and memory usage.\n\nFor 3D data, we use **Conv3D** and **MaxPooling3D** layers to capture spatial and temporal features across multiple frames. These layers enable the model to process video-like data by convolving over three-dimensional volumes. The final layer is a **softmax** activation, which helps in classifying the output into distinct categories, as it's commonly used for multi-class classification problems.\n\nAlternatively, for tasks involving 2D frames with temporal dependencies, we implement a **Conv2D + RNN** model. In this case, **TimeDistributed** is used to apply the **Conv2D** layer independently to each frame in a sequence, followed by an RNN layer (e.g., LSTM or GRU) to capture the temporal dynamics of the sequence. Again, the final layer is a **softmax** activation for classification.\n\nA key consideration while designing the network is to balance accuracy with memory constraints. The model is optimized to achieve good performance with a minimal number of parameters, ensuring that it can efficiently run on memory-limited devices like a webcam or embedded systems.","metadata":{"id":"5_S83V0hNoTQ"}},{"cell_type":"markdown","source":"## Creating Conv3dArchitecture\n\nThe Conv3D model architecture is designed for processing 3D data, such as videos or sequences of images. It uses **Conv3D** layers to capture both spatial and temporal features from the input, where each frame is considered in conjunction with its neighboring frames. To reduce spatial dimensions and retain important features, **MaxPooling3D** layers are applied after the convolutional layers. The output is flattened and passed through fully connected (Dense) layers to learn high-level representations. The final **softmax** layer provides the probability distribution across multiple classes, making it ideal for multi-class video classification tasks.\n","metadata":{"id":"6vvg1xQDUlpt"}},{"cell_type":"code","source":"def Conv3dArchitecture(x, y, z):\n    # Initialize a sequential model\n    conv3d_model = Sequential()\n\n    # First Conv3D block: 64 filters, 3x3x3 kernel, same padding, batch normalization, ReLU activation, and MaxPooling\n    conv3d_model.add(Conv3D(64, (3, 3, 3), strides=(1, 1, 1), padding='same', input_shape=(x, y, z, 3)))\n    conv3d_model.add(BatchNormalization())  # Normalize the activations to stabilize training\n    conv3d_model.add(Activation('relu'))  # Apply ReLU activation function\n    conv3d_model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2)))  # Pool the output with 2x2x2 size\n\n    # Second Conv3D block: 128 filters, 3x3x3 kernel, same padding, batch normalization, ReLU activation, and MaxPooling\n    conv3d_model.add(Conv3D(128, (3, 3, 3), strides=(1, 1, 1), padding='same'))\n    conv3d_model.add(BatchNormalization())  # Normalize activations for better training\n    conv3d_model.add(Activation('relu'))  # Apply ReLU activation function\n    conv3d_model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2)))  # Max pooling layer\n\n    # Third Conv3D block: 256 filters, 3x3x3 kernel, same padding, batch normalization, ReLU activation, and MaxPooling\n    conv3d_model.add(Conv3D(256, (3, 3, 3), strides=(1, 1, 1), padding='same'))\n    conv3d_model.add(BatchNormalization())  # Batch normalization to stabilize learning\n    conv3d_model.add(Activation('relu'))  # Apply ReLU activation function\n    conv3d_model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2)))  # Apply max pooling\n\n    # Fourth Conv3D block: 256 filters, 3x3x3 kernel, same padding, batch normalization, ReLU activation, and MaxPooling\n    conv3d_model.add(Conv3D(256, (3, 3, 3), strides=(1, 1, 1), padding='same'))\n    conv3d_model.add(BatchNormalization())  # Batch normalization for better convergence\n    conv3d_model.add(Activation('relu'))  # Apply ReLU activation function\n    conv3d_model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2)))  # Pooling operation\n\n    # Flatten the output to convert 3D to 1D for dense layers\n    conv3d_model.add(Flatten())\n\n    # Dropout layer to prevent overfitting\n    conv3d_model.add(Dropout(0.5))\n\n    # Fully connected (dense) layer with 512 units and ReLU activation\n    conv3d_model.add(Dense(512, activation='relu'))\n\n    # Dropout layer to further reduce overfitting\n    conv3d_model.add(Dropout(0.5))\n\n    # Output layer with 5 units for 5 classes and softmax activation for multi-class classification\n    conv3d_model.add(Dense(5, activation='softmax'))\n\n    return conv3d_model","metadata":{"id":"6w8CefjbEVgd"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Note:** The next step is to compile the model. When we print the summary of the model, we will see the total number of parameters we have to train.\n","metadata":{"id":"irojaAcfUTyW"}},{"cell_type":"markdown","source":"**Model Summary**\n\n**Note:**\nNumber of parameters depends on number of filters , filter size, number of layers, number of frames in a sequence, Image resolution, number of neurans in dense layers.\n\nThere are 30 frames in a sequence. To classify the video sequence we do not need all the frames.We can reduce the number of training parameters by reducing the number of frames in a sequence. So we will take the alternative frames of a sequence. [0,1,2,4,6,8,10,12,14,16,18,20,22,24,26,27,28,29] We are using 18 frames per video for training and validation.\n","metadata":{"id":"txAfcouX2lBB"}},{"cell_type":"code","source":"# Initialize the SGD optimizer with specific parameters\nsgd = optimizers.SGD(learning_rate=0.001,   # Set the learning rate for optimization\n                     decay=1e-6,            # Set the weight decay for regularization (L2 penalty)\n                     momentum=0.7,          # Set the momentum value to accelerate convergence\n                     nesterov=True)         # Enable Nesterov accelerated gradient for faster convergence\n\n# Define a list of image indices (these can be specific samples or indices for training)\nimg_idx = [0,1,2,4,6,8,10,12,14,16,18,20,22,24,26,27,28,29]\n\n# Length of the image index list\nx = len(img_idx)\n\n# Set height and width of input images (assuming 100x100 images in this example)\ny = 100  # Height of the image\nz = 100  # Width of the image\n\n# Initialize the 3D convolutional model with the defined input shape (x, y, z)\nconv3d_model = Conv3dArchitecture(x, y, z)\n\n# Compile the model using the SGD optimizer, with categorical crossentropy loss for multi-class classification\nconv3d_model.compile(optimizer=sgd,\n                     loss='categorical_crossentropy',  # Loss function for multi-class classification\n                     metrics=['categorical_accuracy'])  # Track categorical accuracy as a performance metric\n\n# Print the summary of the model architecture (shows details like number of layers, parameters, etc.)\nprint(conv3d_model.summary())","metadata":{"id":"Cqp-PFm2U4Va","outputId":"a6321fa4-975c-46ba-d452-0b2103dacfba"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Just to check the file name format\nimport datetime as dt\ncurr_dt_time = dt.datetime.now()\nmodel_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\nprint(curr_dt_time)\nprint(model_name)","metadata":{"id":"d_ga-SyXwBdV","outputId":"d3790b08-90a4-4cfb-c275-621ae27d0a87"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set the time to give unique filename to each checkpoint\nimport datetime\ncurr_dt_time = datetime.datetime.now()\ntrain_path = '/datasets/Project_data/train'\nval_path = '/datasets/Project_data/val'\nnum_train_sequences = len(train_doc)\nprint('# training sequences =', num_train_sequences)\nnum_val_sequences = len(val_doc)\nprint('# validation sequences =', num_val_sequences)\nnum_epochs = 20 # choose the number of epochs\nprint ('# epochs =', num_epochs)","metadata":{"id":"mNLKacmE3DGJ","outputId":"bf502793-646a-4ce3-d139-faeacb086ab6"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Let us save each model epochs in a directory by making check points and also save the model metrics (training accuracy, traing loss ,validation loss, validation accuracy)\nmodel_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n\nif not os.path.exists(model_name):\n    os.mkdir(model_name)\n\nfilepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', save_freq='epoch')\n\nLR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, mode='min', min_delta=0.0001, cooldown=0, min_lr=0.00001)\ncallbacks_list = [checkpoint, LR]","metadata":{"id":"ymvwD1oVtK6Z"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assign the number of steps per epoch for training and validation dataset\nif (num_train_sequences%batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences/batch_size)\nelse:\n    steps_per_epoch = (num_train_sequences//batch_size) + 1\n\nif (num_val_sequences%batch_size) == 0:\n    validation_steps = int(num_val_sequences/batch_size)\nelse:\n    validation_steps = (num_val_sequences//batch_size) + 1","metadata":{"id":"V1d4gQBeu8L4"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Note:** Let us now fit the model. This will start training the model and with the help of the checkpoints, we will be able to save the model at the end of each epoch.","metadata":{"id":"Ho_6hYGYwiVc"}},{"cell_type":"code","source":"# Count the sample dataset for each class\nnum =list(map(lambda x : x.replace('\\n','')[-1], train_doc))\nprint(num.count('0'))\nprint(num.count('1'))\nprint(num.count('2'))\nprint(num.count('3'))\nprint(num.count('4'))","metadata":{"id":"PJwOCgxYqYvy","outputId":"ed48488f-5b1c-4618-dc42-709146d36eee"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Let us create a dummy model to check the Conv3d model architecture is working correcly or not, with small dataset and less epoch**","metadata":{"id":"yrnbJxzS4FqG"}},{"cell_type":"code","source":"# Let us create the train_generator and the val_generator which will be used in .fit_generator.\n\nbatch_size = 32\ny = 100 # (y,z) is the Image resolution\nz = 100\nnormalise = lambda a: a/255.\ntrain_generator = generator(train_path, train_doc[0:160], batch_size,y,z,normalise)\nval_generator = generator(val_path, val_doc[0:40], batch_size,y,z,normalise)\nnum_epochs = 2\n\nhistory = conv3d_model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n                    callbacks=callbacks_list, validation_data=val_generator,\n                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)","metadata":{"id":"GWgs0dj6wcQr","outputId":"55a323ae-f91d-43a3-ae71-1f867f86a5ce"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"These metrics help to evaluate how the model is performing during both training and validation and are useful in identifying issues like overfitting or underfitting.\n\n1. **`history.history['loss']`**: This stores the loss values recorded at the end of each epoch during the training process. The loss value represents the model's error, and lower values indicate better performance.\n   \n2. **`history.history['categorical_accuracy']`**: This stores the categorical accuracy values for each epoch during the training. Categorical accuracy is the percentage of correct predictions out of the total number of predictions made in a multi-class classification problem.\n\n3. **`history.history['val_loss']`**: Similar to training loss, this stores the loss values during the validation phase. This helps monitor how well the model generalizes to unseen data during training.\n\n4. **`history.history['val_categorical_accuracy']`**: This represents the validation categorical accuracy over all epochs. It tells you how accurate the model is when evaluated on the validation dataset after each epoch.","metadata":{"id":"Tpg7zrq_G3bH"}},{"cell_type":"code","source":"# Print the training loss values over all epochs\nprint(\"Training Loss: \", history.history['loss'])\n\n# Print the training categorical accuracy values over all epochs\nprint(\"Training Categorical Accuracy: \", history.history['categorical_accuracy'])\n\n# Print the validation loss values over all epochs\nprint(\"Validation Loss: \", history.history['val_loss'])\n\n# Print the validation categorical accuracy values over all epochs\nprint(\"Validation Categorical Accuracy: \", history.history['val_categorical_accuracy'])","metadata":{"id":"B61IgNeIpoDo","outputId":"140c1805-a84c-495f-8421-ec3830d4b800"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Evaluating Model No. 1\n- **Model**: Model 1 - Conv3D Architecture\n- **Batch Size**: 32\n- **Number of Epochs**: 20\n- **Image Resolution**: 100x100\n- **Normalization Method**: Pixel values divided by 255 (scaling between 0 and 1)","metadata":{"id":"SEMYKk2UiWpu"}},{"cell_type":"markdown","source":"**Set the values of number of epochs, batch_size,Image resolution and number of images in sequence**","metadata":{"id":"Ct6lFL_MvE1c"}},{"cell_type":"code","source":"# Set the time to give unique filename to each checkpoint\ncurr_dt_time = datetime.datetime.now()\n\nnum_epochs = 20 # choose the number of epochs\nbatch_size = 32\n\n# Set the Number of training and validation sequence in dataset\nnum_train_sequences = len(train_doc)\nnum_val_sequences = len(val_doc)\n\nimg_idx = [0,1,2,4,6,8,10,12,14,16,18,20,22,24,26,27,28,29]\nx=len(img_idx) # Number of images in sequence\ny = 100\nz = 100","metadata":{"id":"67niPNkku_zm"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize the SGD optimizer with specific parameters:\n# - learning_rate: Set to 0.001, defining the step size for each update.\n# - decay: Learning rate decay, set to 1e-6 to gradually decrease the learning rate during training.\n# - momentum: Set to 0.7 to help accelerate the gradient descent by using the past gradient's direction.\n# - nesterov: Set to True for Nesterov Accelerated Gradient, which gives a more refined gradient update.\nsgd = optimizers.SGD(learning_rate=0.001, decay=1e-6, momentum=0.7, nesterov=True)\n\n# Create the Conv3D model using the Conv3dArchitecture function, passing the input dimensions (x, y, z).\nconv3d_model1 = Conv3dArchitecture(x, y, z)\n\n# Compile the model, specifying the optimizer (SGD), loss function (categorical_crossentropy),\n# and evaluation metric (categorical_accuracy). This prepares the model for training.\nconv3d_model1.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n\n# Print the model summary to display its architecture, including the number of layers, parameters, etc.\nprint(conv3d_model1.summary())","metadata":{"id":"zH-2vgq9qJIy","outputId":"3d4ea08f-9cc7-4ceb-b575-2cf7e1740b17"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Evaluating Model:**\n- **Callback Directory**: The `callback_directory(curr_dt_time)` function is called to set up directories for saving model checkpoints during training. It creates a unique directory for each training session based on the current time.\n\n- **Steps Per Epoch**: The number of steps per epoch for both the training and validation datasets is calculated based on the total number of sequences divided by the batch size. If the number of sequences isn't perfectly divisible by the batch size, an additional step is added for any remaining data.\n\n- **Normalization**: A simple lambda function is defined for normalizing the images. It divides the pixel values of the images by 255 to scale them to a [0, 1] range, as most models perform better with data that is scaled.\n\n- **Generators**: The `train_generator` and `val_generator` are initialized by calling the `generator()` function, which will yield batches of data for training and validation. This allows for efficient data loading during model training, especially when dealing with large datasets.\n\n- **Model Training**: The model is trained using the `fit()` method, specifying the `train_generator` and `val_generator` for the training and validation data. The `steps_per_epoch` and `validation_steps` define how many batches of data will be processed per epoch for training and validation, respectively. The model will also use any defined callbacks during training.","metadata":{"id":"9rIIkDRnJ543"}},{"cell_type":"code","source":"# Call the callback_directory function to create file paths for saving model checkpoints and logs\n# The function generates unique file names based on the current date and time\ncallback_directory(curr_dt_time)\n\n# Calculate the number of steps per epoch for the training dataset\n# If the total number of training sequences is perfectly divisible by the batch size,\n# use the quotient. Otherwise, add one more step for the remaining data.\nif (num_train_sequences % batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences / batch_size)\nelse:\n    steps_per_epoch = (num_train_sequences // batch_size) + 1\n\n# Similarly, calculate the number of validation steps for the validation dataset\n# If the total number of validation sequences is divisible by the batch size, use the quotient.\n# Otherwise, add one more step for the remaining validation data.\nif (num_val_sequences % batch_size) == 0:\n    validation_steps = int(num_val_sequences / batch_size)\nelse:\n    validation_steps = (num_val_sequences // batch_size) + 1\n\n# Define the normalization function to scale pixel values between 0 and 1\nnormalise = lambda a: a / 255.\n\n# Create the train_generator and val_generator using the generator function.\n# These generators will provide batches of training and validation data to the model during training.\n# The generator function takes care of loading the data, preprocessing it, and yielding batches of data.\ntrain_generator = generator(train_path, train_doc, batch_size, y, z, normalise)\nval_generator = generator(val_path, val_doc, batch_size, y, z, normalise)\n\n# Fit the model using the fit method, specifying the training and validation generators,\n# the number of steps per epoch, the number of epochs, and the callbacks to be used.\n# This trains the model using the data generated by the train_generator and validates it using val_generator.\nhistory_model1 = conv3d_model1.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n                    callbacks=callbacks_list, validation_data=val_generator,\n                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)","metadata":{"id":"8qmSMPIB-EZs","outputId":"320717ec-15ed-4199-c1b3-0ed56bfe6053"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print_model_metrics(\"Model No. 1 - Based on Conv3D Architecture\", history_model1)","metadata":{"id":"F2PPssD_u5L7","outputId":"4567cd07-9e0d-47a3-9e1b-a8942d97a92c"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualizing Model No. 1\nVisualizing the model's performance by plotting training and validation accuracy and loss over epochs. This helps assess how well the model is learning and generalizing, allowing us to identify overfitting or underfitting patterns and tune hyperparameters effectively for better performance.","metadata":{"id":"zDRksaUFkVK9"}},{"cell_type":"code","source":"plot_history(history_model1, num_epochs)","metadata":{"id":"Rf4QDrs2jmWY","outputId":"f812c836-f22a-459b-d21d-ce5ee2077b61"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model No. 1 - Observations\n\n- **Training Loss**: Decreases steadily from 2.7185 (Epoch 1) to 0.9446 (Epoch 20), showing consistent improvement.\n- **Training Accuracy**: Starts at 31.83% (Epoch 1) and gradually increases to 61.09% (Epoch 20), indicating positive training progress.\n- **Validation Loss**: Fluctuates throughout, starting at 1.6146 (Epoch 1) and ending at 2.9742 (Epoch 20), with no significant improvement.\n- **Validation Accuracy**: Begins at 32% (Epoch 1) and ends at 32% (Epoch 20), with minor fluctuations in between.\n- **Learning Rate**: Reduced multiple times via `ReduceLROnPlateau`, from 0.001 to 1e-05, assisting in stabilizing the training.\n- **Model Saving**: Model checkpoints were saved after each epoch with corresponding loss and accuracy values.\n\n### Overall Summary\n\nThe model demonstrates consistent improvement in training accuracy, reaching 61.09% by Epoch 20. However, validation accuracy remains stagnant at around 32%, suggesting potential overfitting. While training loss decreases consistently, the validation loss remains fluctuating. The learning rate reduction helped stabilize training, but the validation performance suggests room for further optimization and generalization.","metadata":{"id":"8YS7B7EfpKuZ"}},{"cell_type":"markdown","source":"### Evaluating Model No. 2\n- **Model**: Model 2 - Conv3D Architecture\n- **Batch Size**: 32\n- **Number of Epochs**: 30\n- **Image Resolution**: 100x100\n- **Normalization Method**: Pixel values divided by 255 (scaling between 0 and 1)","metadata":{"id":"8wOpPVIz55Rj"}},{"cell_type":"code","source":"# Set the time to give unique filename to each checkpoint\ncurr_dt_time = datetime.datetime.now()\n\nnum_epochs = 30 # choose the number of epochs\nbatch_size = 32\n\n# Set the Number of training and validation sequence in dataset\nnum_train_sequences = len(train_doc)\nnum_val_sequences = len(val_doc)\n\nimg_idx = [0,1,2,4,6,8,10,12,14,16,18,20,22,24,26,27,28,29]\nx=len(img_idx) # Number of images in sequence\ny = 100\nz = 100","metadata":{"id":"3xBD6lCl5-H1"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Using SGD optimizer with specified learning rate, decay, momentum, and Nesterov acceleration\nsgd = optimizers.SGD(learning_rate=0.001, decay=1e-6, momentum=0.7, nesterov=True)\n\n# Create the Conv3D model by calling the Conv3dArchitecture function, passing the image dimensions\nconv3d_model2 = Conv3dArchitecture(x, y, z)\n\n# Compile the model with the SGD optimizer, categorical crossentropy loss function,\n# and categorical accuracy as the evaluation metric\nconv3d_model2.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n\n# Print the summary of the model to get an overview of the architecture (layers, parameters, etc.)\nprint(conv3d_model2.summary())","metadata":{"id":"SZaApZKl5-qH","outputId":"cb4af2f7-b537-4897-ef0f-06259b08b162"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Call the callback_directory function to create a unique directory for model checkpoints\n# and other files based on the current date and time.\ncallback_directory(curr_dt_time)\n\n# Assign the number of steps per epoch for training dataset based on the batch size.\n# This determines how many batches of data the model will process in one epoch.\nif (num_train_sequences % batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences / batch_size)  # Exact division of sequences by batch size\nelse:\n    steps_per_epoch = (num_train_sequences // batch_size) + 1  # Handle remainder by adding one extra step\n\n# Similarly, assign the number of steps per epoch for validation dataset.\nif (num_val_sequences % batch_size) == 0:\n    validation_steps = int(num_val_sequences / batch_size)  # Exact division of sequences by batch size\nelse:\n    validation_steps = (num_val_sequences // batch_size) + 1  # Handle remainder by adding one extra step\n\n# Create the train_generator and val_generator using the generator function.\n# These generators will be used to load data in batches for training and validation.\nnormalise = lambda a: a / 255.  # Normalization function to scale pixel values to the range [0, 1]\ntrain_generator = generator(train_path, train_doc, batch_size, y, z, normalise)\nval_generator = generator(val_path, val_doc, batch_size, y, z, normalise)\n\n# Fit the model using the .fit method, which trains the model on the provided data.\n# Pass the train_generator, validation data, and the number of steps for each epoch.\nhistory_model2 = conv3d_model2.fit(\n    train_generator,  # Data generator for training\n    steps_per_epoch=steps_per_epoch,  # Number of batches to process per epoch\n    epochs=num_epochs,  # Total number of epochs to train the model\n    verbose=1,  # Verbosity level of the training process\n    callbacks=callbacks_list,  # List of callbacks (e.g., model checkpoint, learning rate adjustments)\n    validation_data=val_generator,  # Data generator for validation\n    validation_steps=validation_steps,  # Number of validation steps per epoch\n    class_weight=None,  # Weights for each class (None means no specific weighting)\n    workers=1,  # Number of worker threads for data loading\n    initial_epoch=0  # The epoch to start from (use 0 to start from scratch)\n)","metadata":{"id":"SjNkMVaj5-cc","outputId":"5fafe837-dd24-42c2-d275-7b8662d0c8cd"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print_model_metrics(\"Model No. 2 - Based on Conv3D Architecture\", history_model2)","metadata":{"id":"5pbh4_fufn95","outputId":"eb23efe3-450e-4d82-d285-0aa56f951292"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualizing Model No. 2\nVisualizing the model's performance by plotting training and validation accuracy and loss over epochs. This helps assess how well the model is learning and generalizing, allowing us to identify overfitting or underfitting patterns and tune hyperparameters effectively for better performance.","metadata":{"id":"uWP2Z2p3McIP"}},{"cell_type":"code","source":"plot_history(history_model2, num_epochs)","metadata":{"id":"GbuYsi9SMl0X","outputId":"1de15229-83b5-4cd5-fabc-84ec1f371c9d"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model No. 2 - Observations\n\n- **Training Loss**: Decreases steadily from 2.9386 (Epoch 1) to 0.9484 (Epoch 30), showing consistent improvement.\n- **Training Accuracy**: Starts at 31.07% (Epoch 1) and gradually increases to 62.59% (Epoch 30), indicating positive training progress.\n- **Validation Loss**: Fluctuates throughout, starting at 1.6200 (Epoch 1) and ending at 1.0195 (Epoch 30), with noticeable improvements in the later epochs.\n- **Validation Accuracy**: Begins at 21% (Epoch 1) and ends at 56% (Epoch 30), showing a steady upward trend.\n- **Learning Rate**: Reduced multiple times via `ReduceLROnPlateau`, from 0.001 to 1e-05, assisting in stabilizing the training and improving performance.\n- **Model Saving**: Model checkpoints were saved after each epoch with corresponding loss and accuracy values.\n\n### Overall Summary\n\nThis model shows continuous improvement in both training and validation metrics. Training accuracy reaches 62.59% by Epoch 30, while validation accuracy shows steady progress, ending at 56%. The model’s training loss consistently decreases, and validation loss improves significantly over time. The learning rate reduction played a key role in stabilizing the training. Although the validation accuracy is not as high as the training accuracy, the overall trend is positive, suggesting that the model is generalizing better by the end of the training.","metadata":{"id":"Sb0PNL8xExk7"}},{"cell_type":"markdown","source":"# CNN + RNN Stack Architecture\n\nThe **CNN + RNN Stack Architecture** combines the strengths of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) to handle data with both spatial and temporal dependencies. This architecture is particularly effective for tasks like video analysis, image captioning, and speech recognition, where the input contains both spatial features (captured by CNNs) and temporal patterns (captured by RNNs).\n\n- **CNNs** are designed to extract spatial features from input data, such as images or video frames. They use convolutional layers to apply filters that detect local patterns (edges, textures, shapes) and pooling layers to reduce spatial dimensions while preserving important features. These extracted features form the foundation for higher-level reasoning.\n\n- **RNNs**, on the other hand, are ideal for processing sequential data where temporal dependencies exist. They pass information through time, enabling the model to maintain context from previous inputs. This is especially useful when analyzing sequences of images (like video frames) or text. Variants like Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU) address the challenges of learning long-term dependencies.\n\nIn a CNN + RNN stack, CNN layers first extract spatial features, which are then passed to RNN layers to capture the sequential or temporal dependencies, enabling the model to process complex, time-dependent data effectively.","metadata":{"id":"wG3EKoHkxGOj"}},{"cell_type":"markdown","source":"## Creating CNN-LSTM Model\n\nThe **CNN-LSTM model** combines Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks to handle both spatial and temporal data. The CNN component, typically based on a pre-trained model like VGG16, extracts spatial features from individual frames or images. These features are then passed through an LSTM layer, which captures temporal dependencies across sequential data. The TimeDistributed wrapper is used to apply the CNN to each time step in the sequence. The final model includes additional LSTM layers to process temporal patterns, followed by dense layers for classification, making it ideal for tasks like video analysis or time-series prediction.","metadata":{"id":"qNd2wtieu5L9"}},{"cell_type":"code","source":"def cnn_lstm(x, y, z):\n    # Initialize the VGG16 model as the base for feature extraction (without the top classification layers)\n    base_model = VGG16(include_top=False, weights='imagenet', input_shape=(y, z, 3))\n\n    # Add a Flatten layer to convert the output of the base model into a 1D array\n    a = base_model.output\n    a = Flatten()(a)\n\n    # Add a Dense layer for further feature learning\n    features = Dense(64, activation='relu')(a)\n\n    # Create a model for feature extraction\n    conv_model = Model(inputs=base_model.input, outputs=features)\n\n    # Freeze all layers in the base model so their weights won't be updated during training\n    for layer in base_model.layers:\n        layer.trainable = False\n\n    # Build the final sequential model with time-distributed feature extraction\n    model = Sequential()\n    model.add(TimeDistributed(conv_model, input_shape=(x, y, z, 3)))  # Apply conv_model to each time step\n\n    # Add LSTM layers to process the temporal dependencies\n    model.add(LSTM(64, return_sequences=True))  # First LSTM layer returns sequences for further processing\n    model.add(LSTM(32))  # Second LSTM layer outputs the final sequence representation\n\n    # Add a Dense layer for classification (8 output classes)\n    model.add(Dense(8, activation='relu'))\n\n    # Final output layer with softmax for multi-class classification (5 classes)\n    model.add(Dense(5, activation='softmax'))\n\n    return model\n","metadata":{"id":"9L2cNzk0Waga"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Evaluating Model No. 3\n- **Model**: Model 3 - CNN-LSTM Model\n- **Batch Size**: 32\n- **Number of Epochs**: 30\n- **Image Resolution**: 100x100\n- **Optimizer** = SGD","metadata":{"id":"qHv5CeqAQuQb"}},{"cell_type":"code","source":"# Initialize the SGD optimizer with specified parameters\nsgd = optimizers.SGD(learning_rate=0.001,   # Set the learning rate for optimization\n                     decay=1e-6,            # Set weight decay for L2 regularization to prevent overfitting\n                     momentum=0.7,          # Momentum to accelerate convergence by considering previous gradients\n                     nesterov=True)         # Enable Nesterov accelerated gradient for faster convergence\n\n# Initialize the CNN-LSTM model by calling the cnn_lstm function with x, y, and z as input dimensions\nmodel3 = cnn_lstm(x, y, z)\n\n# Compile the model:\n# - Use the SGD optimizer for training\n# - Set 'categorical_crossentropy' as the loss function (for multi-class classification)\n# - Use 'categorical_accuracy' to evaluate the model’s accuracy during training\nmodel3.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n\n# Print the summary of the model architecture to inspect its layers and parameters\nprint(model3.summary())","metadata":{"id":"drNSrv8H-Vy7","outputId":"01eb3d7c-3281-4f5c-bb3a-43f29c7940b1"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get the current date and time to create a unique filename for each checkpoint\ncurr_dt_time = datetime.datetime.now()\n\n# Set the number of epochs for training\nnum_epochs = 30  # You can modify this value based on the number of times you want to train the model\n\n# Define the batch size for training\nbatch_size = 32\n\n# Define the indices of the images in the sequence\nimg_idx = [0, 1, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 27, 28, 29]\nx = len(img_idx)  # Calculate the number of images in the sequence (length of img_idx)\ny = 100  # Height of the input images\nz = 100  # Width of the input images\n\n# Set the number of training and validation sequences in the dataset\nnum_train_sequences = len(train_doc)  # Get the number of training sequences (from the documentation)\nnum_val_sequences = len(val_doc)  # Get the number of validation sequences (from the documentation)\n\n# Calculate the number of steps per epoch for training\n# Steps per epoch determine how many batches the model will process per epoch\nif (num_train_sequences % batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences / batch_size)  # If divisible, use the exact number of steps\nelse:\n    steps_per_epoch = (num_train_sequences // batch_size) + 1  # If not divisible, round up to the next whole number\n\n# Calculate the number of validation steps (similar to steps per epoch)\nif (num_val_sequences % batch_size) == 0:\n    validation_steps = int(num_val_sequences / batch_size)  # If divisible, use the exact number of steps\nelse:\n    validation_steps = (num_val_sequences // batch_size) + 1  # If not divisible, round up to the next whole number","metadata":{"id":"-g_DwS_kab2H"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Call the file path creating callback_dirctory function\ncallbacks_list = callback_directory(curr_dt_time)\n\n# Let us create the train_generator and the val_generator which will be used in .fit_generator.\nnormalise = lambda a: a/255.\ntrain_generator = generator(train_path, train_doc, batch_size,y,z,normalise)\nval_generator = generator(val_path, val_doc, batch_size,y,z,normalise)","metadata":{"id":"3MwyzvXu-avl"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train the model using the fit method\nhistory_model3 = model3.fit(\n    train_generator,               # The training data generator that yields batches of data\n    steps_per_epoch=steps_per_epoch,  # The number of steps (batches) to run per epoch\n    epochs=num_epochs,              # The number of epochs (iterations over the entire dataset)\n    verbose=1,                      # Verbosity mode (1 means progress bar will be shown)\n    callbacks=callbacks_list,       # List of callback functions to apply during training (like checkpoints or early stopping)\n    validation_data=val_generator,  # The validation data generator to evaluate the model after each epoch\n    validation_steps=validation_steps,  # The number of steps (batches) to run for validation per epoch\n    class_weight=None,              # Optionally, you can specify class weights to handle class imbalances (default is None)\n    workers=1,                      # Number of workers for data loading (increase this if you have a large dataset)\n    initial_epoch=0                # The epoch at which to start training (use 0 to start from the beginning)\n)","metadata":{"id":"dXXOsZfi-dfo","outputId":"f254c8bd-d6b8-4eaa-8440-924ff27af3b3"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print_model_metrics(\"Model No. 3 - CNN-LSTM Model\", history_model3)","metadata":{"id":"f1yHqCA2glbR","outputId":"9ca77c06-ee02-4389-9f47-254ec19e05f1"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualizing Model No. 3\nVisualizing the model's performance by plotting training and validation accuracy and loss over epochs. This helps assess how well the model is learning and generalizing, allowing us to identify overfitting or underfitting patterns and tune hyperparameters effectively for better performance.","metadata":{"id":"gSF0CetWacDl"}},{"cell_type":"code","source":"plot_history(history_model3, num_epochs)","metadata":{"id":"dl5Msn9ScAeU","outputId":"005a1e9d-9c6e-4b6b-c961-0ca49d3a74ef"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model No. 3 - Observations\n\n- **Training Loss**: Decreases from 1.6210 (Epoch 1) to 1.3840 (Epoch 30).\n- **Training Accuracy**: Starts at 19.76% (Epoch 1) and increases to 41.78% (Epoch 30).\n- **Validation Loss**: Decreases from 1.6120 (Epoch 1) to 1.3984 (Epoch 30).\n- **Validation Accuracy**: Starts at 20.06% (Epoch 1) and increases to 34.00% (Epoch 30).\n- **Learning Rate**: Constant at 0.001 throughout all epochs.\n- **Model Saving**: Model checkpoints were saved after each epoch with corresponding loss and accuracy values.\n\n### Overall Summary\n\nThe model demonstrates steady improvement in both training and validation accuracy, with training accuracy reaching 41.78% by Epoch 30 and validation accuracy at 34%. Both training and validation losses show consistent decreases, but the validation accuracy lags behind the training accuracy, suggesting potential overfitting. The learning rate remained fixed at 0.001 throughout, aiding in the smooth progression of training. Further optimization might be required to bridge the gap between training and validation performance, particularly in terms of generalization.","metadata":{"id":"uAvLYCMhR44l"}},{"cell_type":"markdown","source":"## Creating CNN-GRU with Transfer Learning\nThe **CNN-GRU** architecture combines Convolutional Neural Networks (CNNs) and Gated Recurrent Units (GRUs) to process both spatial and temporal information. CNNs are used for feature extraction from images, while GRUs capture temporal dependencies in sequential data, making this architecture ideal for tasks like video classification or time-series prediction. **Transfer Learning** enhances this model by utilizing a pre-trained CNN, such as VGG16, to extract meaningful features from images without requiring extensive training from scratch. This approach leverages knowledge learned from large datasets, improving the model’s performance and reducing training time for specific tasks with limited data.","metadata":{"id":"u2LJtUadu5MA"}},{"cell_type":"code","source":"def cnn_gru(x, y, z, dropout):\n    \"\"\"\n    This function creates a CNN-GRU model with pre-trained VGG16 as the feature extractor.\n\n    Arguments:\n    x -- number of time steps in the input sequence\n    y -- height of the input image\n    z -- width of the input image\n    dropout -- dropout rate to apply for regularization\n\n    Returns:\n    model -- the compiled CNN-GRU model\n    \"\"\"\n\n    # Load the pre-trained VGG16 model (without the top classification layers)\n    base_model = VGG16(include_top=False, weights='imagenet', input_shape=(y, z, 3))\n\n    # Flatten the output of the VGG16 base model\n    a = base_model.output\n    a = Flatten()(a)  # Flatten to convert the 2D feature maps into a 1D vector\n\n    # Add a Dense layer for feature learning with 64 neurons and ReLU activation\n    features = Dense(64, activation='relu')(a)\n\n    # Define the convolutional model (CNN feature extractor)\n    conv_model = Model(inputs=base_model.input, outputs=features)\n\n    # Freeze the layers of the base VGG16 model so their weights are not updated during training\n    for layer in base_model.layers:\n        layer.trainable = False\n\n    # Build the final sequential model with CNN-GRU architecture\n    model = Sequential()\n\n    # Apply the TimeDistributed wrapper to the convolutional model to process each time step\n    model.add(TimeDistributed(conv_model, input_shape=(x, y, z, 3)))\n\n    # Add a GRU layer with 128 units and return sequences to pass the output to the next layer\n    model.add(GRU(128, return_sequences=True))\n\n    # Add a Dropout layer to reduce overfitting (dropout rate is passed as an argument)\n    model.add(Dropout(dropout))\n\n    # Add another GRU layer with 64 units to capture temporal dependencies\n    model.add(GRU(64))\n\n    # Add another Dropout layer for regularization\n    model.add(Dropout(dropout))\n\n    # Add a Dense layer with 64 units and ReLU activation for further processing of features\n    model.add(Dense(64, activation='relu'))\n\n    # Add another Dropout layer to further prevent overfitting\n    model.add(Dropout(dropout))\n\n    # Add the final output layer with 5 units (for 5-class classification) and softmax activation\n    model.add(Dense(5, activation='softmax'))\n\n    # Return the final model\n    return model","metadata":{"id":"gHpLPf8xu5MB"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Key Points**\n1. **VGG16 as Feature Extractor**:\n   - The model uses the pre-trained `VGG16` model, excluding its top classification layers (`include_top=False`). This allows the model to use `VGG16` as a feature extractor while the classification layers are custom-built.\n   \n2. **TimeDistributed Layer**:\n   - The `TimeDistributed` wrapper allows the convolutional model to process each time step of the sequence independently. This is useful for processing sequences of images (like frames in a video).\n\n3. **GRU Layers**:\n   - The model uses two **GRU (Gated Recurrent Unit)** layers to capture temporal dependencies in the sequence of image features. The first GRU layer returns sequences to provide input for the second GRU layer, which processes the entire sequence.\n\n4. **Dropout for Regularization**:\n   - Dropout layers are added after each GRU and Dense layer to prevent overfitting by randomly setting a fraction of input units to 0 during training. The dropout rate is passed as a parameter (`dropout`).\n\n5. **Dense Output Layer**:\n   - The final Dense layer has 5 units, corresponding to a 5-class classification problem, with softmax activation to output probabilities for each class.\n\nThis model is suitable for tasks like video classification, where temporal dependencies between frames need to be captured, and spatial features from each frame need to be extracted.","metadata":{"id":"pKm9pNQHu5MB"}},{"cell_type":"markdown","source":"### Evaluating Model No. 4\n- **Model**: Model 4 - CNN-GRU with Transfer learning using VGG16 imagenet\n- **Batch Size**: 32\n- **Number of Epochs**: 20\n- **Image Resolution**: 100x100\n- **Optimizer**: Adam\n- **Normalization Method**: Pixel values divided by 255 (scaling between 0 and 1)","metadata":{"id":"ZhhhOsYoxNrY"}},{"cell_type":"code","source":"# Initialize the Adam optimizer\n# Adam optimizer is a popular choice due to its adaptive learning rate and good performance on many tasks\nadam = optimizers.Adam(learning_rate=0.0001)\n\n# Create the CNN-GRU model using the cnn_gru function with input dimensions (x, y, z) and a dropout rate of 0.25\n# This function combines CNN for feature extraction with GRU for sequential learning\nmodel4 = cnn_gru(x, y, z, 0.25)\n\n# Compile the model:\n# - Use Adam optimizer for training the model\n# - Specify 'categorical_crossentropy' as the loss function for multi-class classification\n# - Track 'categorical_accuracy' during training to evaluate the model's performance\nmodel4.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n\n# Print the model summary to inspect the architecture, including the layers and the number of parameters\nprint(model4.summary())","metadata":{"id":"s8mj4CPdF5ZK","outputId":"e75dcfa2-c873-4b66-8838-cf753456fe9c"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get the current date and time to create a unique filename for each checkpoint\ncurr_dt_time = datetime.datetime.now()\n\n# Define the number of epochs for training\nnum_epochs = 20  # This is the number of times the model will iterate over the entire training dataset\n\n# Define the batch size for training\nbatch_size = 32  # The number of samples processed before the model is updated during training\n\n# Calculate the number of training and validation sequences from the training and validation documentation\nnum_train_sequences = len(train_doc)  # Number of sequences in the training dataset\nnum_val_sequences = len(val_doc)  # Number of sequences in the validation dataset\n\n# Define the image indices used in the sequence\nimg_idx = [0, 1, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 27, 28, 29]\nx = len(img_idx)  # Calculate the number of images in the sequence (length of img_idx)\ny = 100  # Height of the input images\nz = 100  # Width of the input images\n\n# Calculate the number of steps per epoch for training\n# Steps per epoch is the number of batches that will be processed in each epoch\nif (num_train_sequences % batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences / batch_size)  # If evenly divisible, use the exact number of steps\nelse:\n    steps_per_epoch = (num_train_sequences // batch_size) + 1  # If not evenly divisible, round up to the next whole number\n\n# Calculate the number of validation steps for the validation dataset\n# Validation steps determine how many validation batches will be processed per epoch\nif (num_val_sequences % batch_size) == 0:\n    validation_steps = int(num_val_sequences / batch_size)  # If evenly divisible, use the exact number of steps\nelse:\n    validation_steps = (num_val_sequences // batch_size) + 1  # If not evenly divisible, round up to the next whole number","metadata":{"id":"UtciYMwOF0K-"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Call the function to create the callback directory path\n# This function generates a directory path to store model checkpoints and logs\ncallbacks_list = callback_directory(curr_dt_time)\n\n# Define a normalization function to scale image pixel values to the range [0, 1]\nnormalise = lambda a: a / 255.0  # Dividing pixel values by 255 to normalize them\n\n# Create the training data generator using the 'generator' function\n# This generator will yield batches of training data from the 'train_path' and 'train_doc'\n# It takes the training data path, documentation, batch size, image dimensions, and normalization function\ntrain_generator = generator(\n    train_path,         # Path to the training dataset\n    train_doc,          # Documentation for the training dataset (contains image file paths and labels)\n    batch_size,         # Batch size for each training iteration\n    y,                  # Height of the input images\n    z,                  # Width of the input images\n    normalise           # Normalization function to scale the pixel values\n)\n\n# Create the validation data generator using the 'generator' function\n# This generator will yield batches of validation data from the 'val_path' and 'val_doc'\n# It takes the validation data path, documentation, batch size, image dimensions, and normalization function\nval_generator = generator(\n    val_path,           # Path to the validation dataset\n    val_doc,            # Documentation for the validation dataset (contains image file paths and labels)\n    batch_size,         # Batch size for each validation iteration\n    y,                  # Height of the input images\n    z,                  # Width of the input images\n    normalise           # Normalization function to scale the pixel values\n)","metadata":{"id":"a23k4JMQF1FG"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train the model using the .fit method\n# 'train_generator' and 'val_generator' will provide batches of training and validation data, respectively.\n\nhistory_model4 = model4.fit(\n    train_generator,                # The training data generator that yields batches of training data\n    steps_per_epoch=steps_per_epoch,  # Number of steps (batches) to process in each epoch for training\n    epochs=num_epochs,               # Total number of epochs (iterations over the entire dataset)\n    verbose=1,                       # Verbosity level; 1 will display a progress bar for each epoch\n    callbacks=callbacks_list,        # List of callbacks to apply during training (e.g., checkpointing, early stopping)\n    validation_data=val_generator,  # The validation data generator to evaluate the model after each epoch\n    validation_steps=validation_steps,  # Number of validation batches to process in each epoch\n    class_weight=None,               # Class weights (None means no weighting; you can specify weights for imbalanced classes)\n    workers=1,                       # Number of workers for data loading; increase if you have a large dataset\n    initial_epoch=0                  # The epoch at which to start training (use 0 to start fresh, or specify to resume from a checkpoint)\n)","metadata":{"id":"i2gwepm7F6QV","outputId":"1d431fb5-839c-4e37-ca35-636ae19bec7e"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print_model_metrics(\"Model No. 4 - CNN-GRU with Transfer Learning\", history_model4)","metadata":{"id":"5DWprzTQjRD_","outputId":"24b03185-cafc-438c-df03-c6c156cf1188"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualizing Model No. 4\nVisualizing the model's performance by plotting training and validation accuracy and loss over epochs. This helps assess how well the model is learning and generalizing, allowing us to identify overfitting or underfitting patterns and tune hyperparameters effectively for better performance.","metadata":{"id":"Ck-UJU5Vu5MD"}},{"cell_type":"code","source":"plot_history(history_model4, num_epochs)","metadata":{"id":"zJ41VGYEGAxm","outputId":"3ff144ba-1878-4660-bf88-dd406cfa1a9f"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model No. 4 - Observations\n\n- **Training Accuracy**: The model's training accuracy increased steadily from 22.93% to 95.78% over 20 epochs.\n- **Validation Accuracy**: Validation accuracy improved from 28% to 80%.\n- **Loss**: The model's loss decreased from 1.5937 to 0.2140.\n- **Learning Rate**: The learning rate was reduced at Epoch 17 from 0.0001 to 0.00005 via `ReduceLROnPlateau`.\n- **Model Saving**: Model checkpoints were saved after each epoch.\n\n### Overall Summary\n\nThe model demonstrates consistent improvement in both training and validation accuracy, with a noticeable reduction in loss over the 20 epochs. The training accuracy rose from 22.93% to 95.78%, and validation accuracy increased from 28% to 80%. The learning rate was adjusted in the later epochs to enhance the model's performance, particularly at Epoch 17. Despite the validation accuracy improving steadily, there is still a gap between training and validation accuracy, suggesting that there may be some room for further improvement.","metadata":{"id":"238gcZWs15BJ"}},{"cell_type":"markdown","source":"## CNN, LSTM and GRU with Transfer Learning\n\nThe model combining **CNN (Convolutional Neural Networks)**, **LSTM (Long Short-Term Memory)**, and **GRU (Gated Recurrent Units)** with **Transfer Learning** is designed to handle complex tasks involving both spatial and temporal data. **CNNs** are used for feature extraction from images, automatically learning hierarchical patterns. **LSTMs** and **GRUs**, both types of recurrent neural networks, capture temporal dependencies in sequential data, making them ideal for time-series prediction or video classification. **Transfer Learning** leverages pre-trained CNN models (like VGG16 or ResNet) on large datasets, allowing the model to benefit from previously learned features, reducing training time and improving performance, especially on smaller datasets. This architecture is particularly powerful for applications such as video processing, speech recognition, and sequence-based predictions.","metadata":{"id":"22qYmZA8u5ME"}},{"cell_type":"code","source":"def cnn_lstm_gru(x, y, z, dropout):\n    \"\"\"\n    This function builds a deep learning model combining CNN (VGG16), LSTM, and GRU layers.\n    CNN is used for feature extraction from images, while LSTM and GRU capture temporal dependencies\n    from sequential data. Dropout layers are added to reduce overfitting.\n\n    Parameters:\n    - x: Number of images in the sequence.\n    - y: Height of the input image.\n    - z: Width of the input image.\n    - dropout: Dropout rate to apply for regularization in LSTM/GRU layers and Dense layers.\n\n    Returns:\n    - A compiled Keras model combining CNN, LSTM, and GRU layers.\n    \"\"\"\n\n    # Load the pre-trained VGG16 model without the fully connected layers (include_top=False)\n    # Weights are set to 'imagenet', which are pre-trained weights learned on ImageNet dataset.\n    # The input shape is set to (y, z, 3), corresponding to height, width, and 3 color channels (RGB).\n    base_model = VGG16(include_top=False, weights='imagenet', input_shape=(y, z, 3))\n\n    # Flatten the output from the base model to create a 1D vector from the image features.\n    a = base_model.output\n    a = Flatten()(a)\n\n    # Add a Dense layer to extract features from the flattened image data.\n    # This layer has 64 units and uses ReLU activation function.\n    features = Dense(64, activation='relu')(a)\n\n    # Create a new model for the CNN feature extraction part.\n    conv_model = Model(inputs=base_model.input, outputs=features)\n\n    # Freeze all the layers in the base VGG16 model so that they will not be updated during training.\n    for layer in base_model.layers:\n        layer.trainable = False\n\n    # Start building the Sequential model which will contain CNN, LSTM, GRU, and Dense layers.\n    model = Sequential()\n\n    # Apply the conv_model to each image in the sequence using TimeDistributed wrapper.\n    # This will process each image independently using the same CNN model.\n    model.add(TimeDistributed(conv_model, input_shape=(x, y, z, 3)))\n\n    # Add an LSTM layer with 128 units to capture long-term dependencies in the sequence.\n    # 'return_sequences=True' ensures that the output will be passed on to the next layer in the sequence.\n    model.add(LSTM(128, return_sequences=True))\n\n    # Add a Dropout layer with the specified dropout rate to reduce overfitting.\n    model.add(Dropout(dropout))\n\n    # Add a GRU layer with 64 units to capture more temporal dependencies in the sequence.\n    # 'return_sequences=True' is set to provide sequences to the next layer.\n    model.add(GRU(64, return_sequences=True))\n\n    # Add another Dropout layer to further prevent overfitting.\n    model.add(Dropout(dropout))\n\n    # Add another GRU layer with 32 units to continue capturing sequential patterns.\n    # This layer outputs a 1D vector (no sequences) for the final Dense layers.\n    model.add(GRU(32))\n\n    # Add a Dense layer with 64 units and ReLU activation to further process the features.\n    model.add(Dense(64, activation='relu'))\n\n    # Add a Dropout layer for regularization.\n    model.add(Dropout(dropout))\n\n    # Add a final Dense layer with 5 units and softmax activation for classification.\n    # The softmax function will output a probability distribution across 5 classes.\n    model.add(Dense(5, activation='softmax'))\n\n    # Return the built model.\n    return model","metadata":{"id":"f1L75sGQscuv"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Key Points**\n1. **Base Model (VGG16)**: The function uses the VGG16 model, which is a pre-trained Convolutional Neural Network (CNN) typically used for image classification tasks. We exclude the top fully connected layers and use the convolutional layers to extract features from the input images.\n\n2. **Feature Extraction (Flatten + Dense)**: The output from the VGG16 model is flattened into a 1D vector and passed through a Dense layer with 64 units to further extract features.\n\n3. **TimeDistributed Layer**: Since the input is a sequence of images (such as frames in a video), we use `TimeDistributed` to apply the CNN model to each image in the sequence. This ensures that each image is processed independently, but the same model weights are used for all images.\n\n4. **LSTM and GRU Layers**: These layers are used to capture temporal dependencies in the sequential data.\n   - LSTM (Long Short-Term Memory) helps capture long-range dependencies by maintaining memory over time.\n   - GRU (Gated Recurrent Unit) is similar to LSTM but with fewer parameters and faster computation.\n\n5. **Dropout**: Dropout layers are added after LSTM, GRU, and Dense layers to prevent overfitting by randomly setting some of the layer outputs to zero during training.\n\n6. **Output Layer**: The final output layer has 5 units and uses softmax activation to perform multi-class classification. It will output a probability distribution over the 5 classes.\n\nThis model is suitable for tasks such as video classification or sequential image processing, where both spatial and temporal features need to be captured.","metadata":{"id":"5iVrlt_nu5ME"}},{"cell_type":"code","source":"# Import datetime library to set the current timestamp for unique filenames\nimport datetime\n\n# Set the current date and time to generate a unique filename for each checkpoint\ncurr_dt_time = datetime.datetime.now()\n\n# Set the number of epochs for training\nnum_epochs = 30  # Choose the number of epochs (iterations over the entire dataset)\n\n# Set the batch size for training (number of samples per batch)\nbatch_size = 32\n\n# Define the indices of images used in the sequence for training/validation\nimg_idx = [0,1,2,4,6,8,10,12,14,16,18,20,22,24,26,27,28,29]\nx = len(img_idx)  # Number of images in sequence (length of img_idx list)\n\n# Define the dimensions of the input images (height and width)\ny = 100  # Height of input images\nz = 100  # Width of input images\n\n# Determine the number of sequences (videos or time-steps) in the training and validation datasets\nnum_train_sequences = len(train_doc)  # Number of training sequences (documents or data points)\nnum_val_sequences = len(val_doc)  # Number of validation sequences (documents or data points)\n\n# Calculate the number of steps per epoch for training based on the total number of training sequences\n# Steps per epoch determine how many batches of data are processed before one full epoch is complete\nif (num_train_sequences % batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences / batch_size)  # Exact division\nelse:\n    steps_per_epoch = (num_train_sequences // batch_size) + 1  # If not divisible, add an extra step for the remaining samples\n\n# Similarly, calculate the number of validation steps per epoch for the validation data\nif (num_val_sequences % batch_size) == 0:\n    validation_steps = int(num_val_sequences / batch_size)  # Exact division\nelse:\n    validation_steps = (num_val_sequences // batch_size) + 1  # Add an extra step for the remaining samples\n","metadata":{"id":"SyVZA4lc_1Gv"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Evaluating Model No. 5\n- **Model**: Model 5 - CNN, LSTM and GRU with Transfer learning\n- **Batch Size**: 32\n- **Number of Epochs**: 30\n- **Image Resolution**: 100x100\n- **Optimizer**: Adam\n- **Normalization Method**: Pixel values divided by 255 (scaling between 0 and 1)","metadata":{"id":"7eI908vWsVcO"}},{"cell_type":"code","source":"# Importing the Adam optimizer from Keras\nfrom tensorflow.keras import optimizers\n\n# Compile the conv3d_model\n# Using Adam optimizer with a custom learning rate\nadam = optimizers.Adam(learning_rate=0.0001)  # Initialize the Adam optimizer with a learning rate of 0.0001\n\n# Create the model using the cnn_lstm_gru function\n# The model architecture combines CNN for feature extraction, LSTM, and GRU for processing sequential data\n# The dropout parameter (0.25) is used to reduce overfitting during training\nmodel5 = cnn_lstm_gru(x, y, z, 0.25)  # Create the model with the specified input shape and dropout rate\n\n# Compile the model with the following settings:\n# - optimizer: Adam optimizer with a very small learning rate to fine-tune the model more carefully\n# - loss function: 'categorical_crossentropy', suitable for multi-class classification tasks\n# - metrics: 'categorical_accuracy', to track the accuracy for multi-class classification\nmodel5.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n\n# Print the model summary to display the architecture of the model\n# This shows all the layers, their shapes, and the number of parameters in the model\nprint(model5.summary())  # Display the summary of the model. Note: model7_2 should likely be model5 for consistency","metadata":{"id":"UXOycvq8_1Sd","outputId":"3e95c97d-b5d7-4baf-cd46-68836f65289a"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Call the file path creating callback_directory function\n# This function is responsible for creating directories for saving checkpoints and other outputs during training\ncallbacks_list = callback_directory(curr_dt_time)  # Get the list of callbacks, including model checkpointing or early stopping\n\n# Let us create the train_generator and the val_generator which will be used in .fit_generator.\n# These generators provide batches of training and validation data during the training process.\n\nnormalise = lambda a: a / 255.  # Define a normalization function that scales pixel values to the range [0, 1]\n# This function divides each pixel value by 255 to normalize the images from [0, 255] to [0, 1]\n\n# Create the training data generator using the generator function\n# The generator will yield batches of images and corresponding labels for training\ntrain_generator = generator(\n    train_path,        # Path to the training dataset\n    train_doc,         # Documentation containing information about the training data (e.g., file names, labels)\n    batch_size,        # Batch size for loading data (number of samples per batch)\n    y,                 # Image height (dimensions of the input images)\n    z,                 # Image width (dimensions of the input images)\n    normalise          # Normalization function to scale the image pixel values\n)\n\n# Create the validation data generator using the same generator function for validation data\n# It works similarly to the train_generator but uses the validation data\nval_generator = generator(\n    val_path,          # Path to the validation dataset\n    val_doc,           # Documentation containing information about the validation data\n    batch_size,        # Batch size for validation data\n    y,                 # Image height\n    z,                 # Image width\n    normalise          # Normalization function\n)","metadata":{"id":"T1lFH276_1Zt"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Key Points**\n1. **callbacks_list**: This variable holds the list of callbacks, which are functions that are triggered during the training process. Callbacks can perform tasks like saving the model after each epoch, implementing early stopping, or logging training metrics. The `callback_directory(curr_dt_time)` function creates directories for saving checkpoints or other files based on the current date and time.\n\n2. **normalise**: A simple lambda function that normalizes image pixel values by dividing them by 255. This ensures that the pixel values are in the range [0, 1], which is often better for training deep learning models.\n\n3. **train_generator**: This generator loads batches of training data, applies the normalization function, and feeds them to the model during training. It is created using the `generator` function, which is expected to handle the loading, processing, and batching of data.\n\n4. **val_generator**: Similar to `train_generator`, but this one feeds validation data to the model for evaluation during training. It helps monitor the model's performance on unseen data after each epoch.\n\nThese generators are key for efficiently feeding data to the model during training, especially for large datasets that cannot be loaded all at once.","metadata":{"id":"-kRDHv9_u5MH"}},{"cell_type":"code","source":"#Fit the model\nhistory_model5 = model5.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n                    callbacks=callbacks_list, validation_data=val_generator,\n                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)","metadata":{"id":"w31rjplRAZbJ","outputId":"ec059f2b-81c6-4f06-ef7e-998e8a46d0a5"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print_model_metrics(\"Model No. 5 - CNN, LSTM and GRU with Transfer Learning\", history_model5)","metadata":{"id":"P0QMtYRqBvIa","outputId":"4502f61d-c7e2-4fd4-9cfd-0defc80e2641"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualizing Model No. 5\nVisualizing the model's performance by plotting training and validation accuracy and loss over epochs. This helps assess how well the model is learning and generalizing, allowing us to identify overfitting or underfitting patterns and tune hyperparameters effectively for better performance.","metadata":{"id":"pAHQnho0u5MH"}},{"cell_type":"code","source":"plot_history(history_model5, num_epochs)","metadata":{"id":"vFAxrHLIAaAG","outputId":"286d8f2c-070e-4d12-f01e-77d6475b84fa"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model No. 5 - Observations\n\n- **Training Accuracy**: Improved significantly, reaching 98.49% by epoch 30.\n- **Validation Accuracy**: Trended upward, ending at 75.00%.\n- **Loss**: Training loss steadily decreased, while validation loss showed periodic improvements and fluctuations.\n- **Learning Rate**: Reduced multiple times using `ReduceLROnPlateau` to optimize performance.\n- **Model Saving**: Saved after each epoch, with the model showing consistent improvements in training accuracy and validation accuracy until it plateaued.\n\n**Overall Summary**\n\nThis model demonstrated steady improvement in training accuracy, which reached 98.49% by the final epoch. The validation accuracy showed an upward trend, though with some fluctuations, and finished at 75.00%. While the validation loss showed slight improvements throughout the epochs, it also experienced some increases. The model's learning rate was adjusted using `ReduceLROnPlateau` to prevent overfitting. Despite a few plateaus, the model performed well and was saved after each epoch to capture its best states during training.","metadata":{"id":"gJNwqq_B5NVI"}},{"cell_type":"markdown","source":"# Data Augmentation\n\n**Enhancing Model Accuracy with Augmentation Techniques**\n\nData augmentation is a technique used in machine learning and deep learning to increase the diversity of training data without actually collecting new data. It involves applying various transformations to the existing dataset to create modified versions of the original data. This helps improve the robustness and generalization of models by providing them with more varied examples to learn from, reducing the risk of overfitting.\n\nCommon data augmentation methods include:\n- **For image data:** Rotation, scaling, flipping, cropping, color adjustment, and adding noise.\n- **For text data:** Synonym replacement, random insertion, deletion, or swapping of words, and paraphrasing.\n- **For audio data:** Time stretching, pitch shifting, noise injection, and changes in speed or volume.\n\nData augmentation is especially beneficial when working with limited data, as it can create a broader range of training examples from the same original data.","metadata":{"id":"V-n09S9tjWbI"}},{"cell_type":"code","source":"# Open the image from the specified folder path\nim = Image.open(folder_name + '/train/' + train_doc[9].split(';')[0] + '/' + imgs[0])\n\n# Display the image using matplotlib\nplt.imshow(im)\n\n# Print the size (dimensions) of the image\nprint(im.size)","metadata":{"id":"x8gpnNMRnSvl","outputId":"1dd507c6-b5a9-486d-9636-5b7095031d4a"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Apply a random affine transformation (shifting the image) with random horizontal and vertical shifts\nshifted = im.transform(im.size, Image.AFFINE, (1, 0, np.random.randint(-30, 30), 0, 1, np.random.randint(-30, 30)))\n\n# Convert the transformed image to grayscale\ngray = shifted.convert('L')\n\n# Convert the grayscale image to a NumPy array of type float32\nimage = np.asarray(gray).astype(np.float32)\n\n# Find the minimum and maximum coordinates of non-black pixels to define the bounding box\nx0, y0 = np.argwhere(image > 0).min(axis=0)\nx1, y1 = np.argwhere(image > 0).max(axis=0)\n\n# Crop the image based on the bounding box to remove any unnecessary space around the object\ncropped = shifted.crop((y0, x0, y1, x1))\n\n# Resize the cropped image to 100x100 pixels\nim_resized = cropped.resize(size=(100, 100))\n\n# Display the resized image\nplt.imshow(im_resized)\n\n# Print the coordinates of the bounding box around the non-black pixels with labels for clarity\nprint(f\"Bounding box coordinates: x0 = {x0}, y0 = {y0}, x1 = {x1}, y1 = {y1}\")","metadata":{"id":"9buRruEDndWQ","outputId":"dc3f7f5d-54ef-493b-f153-5754ced2870b"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display the three images (original grayscale, shifted image, and cropped/resized image) horizontally\n\n# Create a subplot with 1 row and 3 columns\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Show the grayscale image\naxes[0].imshow(gray)\naxes[0].set_title('Grayscale Image')\n\n# Show the shifted image\naxes[1].imshow(shifted)\naxes[1].set_title('Shifted Image')\n\n# Show the cropped and resized image\naxes[2].imshow(im_resized)\naxes[2].set_title('Cropped & Resized Image')\n\n# Hide the axes for a cleaner look\nfor ax in axes:\n    ax.axis('off')\n\n# Display the images\nplt.show()","metadata":{"id":"GpkGddlbu5MI","outputId":"2a1e4def-e503-44d6-f1c3-7b872539d922"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Image Augmentation Function\ndef image_augment(image, y, z):\n    # Apply a random affine transformation (shifting the image) with random horizontal and vertical shifts\n    shifted = image.transform(image.size, Image.AFFINE, (1, 0, np.random.randint(-30, 30), 0, 1, np.random.randint(-30, 30)))\n\n    # Convert the transformed image to grayscale\n    gray = shifted.convert('L')\n\n    # Convert the grayscale image to a NumPy array with float32 type\n    im = np.asarray(gray).astype(np.float32)\n\n    # Find the minimum and maximum coordinates of non-black pixels to define the bounding box\n    x0, y0 = np.argwhere(im > 0).min(axis=0)\n    x1, y1 = np.argwhere(im > 0).max(axis=0)\n\n    # Crop the image based on the bounding box to remove any unnecessary space around the object\n    cropped = shifted.crop((y0, x0, y1, x1))\n\n    # Resize the cropped image to the specified dimensions (y, z)\n    im_resized = np.asarray(cropped.resize(size=(y, z))).astype(np.float32)\n\n    # Return the augmented and resized image as a NumPy array\n    return im_resized","metadata":{"id":"drzm95r-jUXo"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Creating Generator with Data Augmentation Techniques\n\n**Detailed Explanation of the `generator_with_aug` Function**\n\nThe function `generator_with_aug` is a data generator used for efficiently loading and processing images in batches, with an optional augmentation process. This function is particularly useful when dealing with large datasets that cannot fit entirely into memory. It dynamically loads and preprocesses data in batches, yielding the data and corresponding labels as required by machine learning models, especially in deep learning applications.\n\nHere’s a breakdown of the function:\n\n---\n\n#### **Function Definition**\n\n```python\ndef generator_with_aug(source_path, folder_list, batch_size, y, z, normalise, augment=False):\n```\n\n- **`source_path`**: Path to the root directory where images are stored.\n- **`folder_list`**: List of folder names, each containing images from a specific video or class.\n- **`batch_size`**: The number of samples per batch.\n- **`y, z`**: The target image resolution for resizing the images (height and width).\n- **`normalise`**: A function for normalizing the image pixel values (e.g., rescaling them to the range [0,1]).\n- **`augment`**: A boolean flag that specifies whether data augmentation should be applied (default is `False`).\n\n---\n\n#### **Initial Setup and Variables**\n\n```python\nimg_idx = [0, 1, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 27, 28, 29]  # selected image indices\nx = len(img_idx)  # the number of images per video\n```\n\n- **`img_idx`**: A predefined list of indices that specify which frames (images) to use from each video.\n- **`x`**: The number of images (frames) to process per video, which is equal to the length of `img_idx`.\n\n---\n\n#### **Main Loop and Batch Processing**\n\nThe function uses an infinite loop (`while True:`) to yield batches of data continuously, making it suitable for training deep learning models with large datasets.\n\n```python\nt = np.random.permutation(folder_list)  # Randomly shuffle the folder list\nnum_batches = int(len(t) / batch_size)  # Calculate the number of batches\n```\n\n- **`t`**: A shuffled list of folder names to ensure randomness in batch generation.\n- **`num_batches`**: The total number of full batches that can be created from the dataset, based on the specified batch size.\n\n---\n\n#### **Batch Creation**\n\nFor each batch, the function processes the images from the folders and creates the batch data (`batch_data`) and corresponding labels (`batch_labels`).\n\n```python\nbatch_data = np.zeros((batch_size, x, y, z, 3))  # Initialize an empty array to store image data\nbatch_labels = np.zeros((batch_size, 5))  # Initialize an empty array for one-hot encoded labels\n```\n\n- **`batch_data`**: A 5D NumPy array to store the image data in the format `(batch_size, num_images, height, width, channels)`.\n- **`batch_labels`**: A 2D array to store the one-hot encoded labels corresponding to the images.\n\n---\n\n#### **Image Augmentation (Optional)**\n\nIf **augmentation is enabled**, additional augmented data is created alongside the original data:\n\n```python\nif augment:\n    batch_data_aug = np.zeros((batch_size, x, y, z, 3))  # Array to store augmented images\n```\n\n- **`batch_data_aug`**: A 5D NumPy array to store augmented image data.\n\nFor each image, random affine transformations (e.g., shifts, cropping) and resizing are applied:\n\n```python\nim_aug = image_augment(image, y, z)\n```\n\n- **`image_augment`**: A function that applies random transformations (such as translation, resizing, and cropping) to the image for data augmentation.\n\n---\n\n#### **Image Processing**\n\nFor each folder in the batch:\n\n1. **Image Loading and Processing**:\n   - The images are loaded from the specified folder.\n   - The images are resized to the target resolution `(y, z)`.\n   - If the image size is 160 pixels wide, the function crops the image to a 120x120 region.\n\n2. **Normalizing**:\n   - Each color channel (RGB) of the image is normalized separately using the provided `normalise` function.\n\n3. **Augmentation**:\n   - If augmentation is enabled, the augmented images are processed in the same way as the original images and stored in `batch_data_aug`.\n\n```python\nbatch_data[folder, idx, :, :, 0] = normalise(image[:, :, 0])  # Normalize the red channel\nbatch_data[folder, idx, :, :, 1] = normalise(image[:, :, 1])  # Normalize the green channel\nbatch_data[folder, idx, :, :, 2] = normalise(image[:, :, 2])  # Normalize the blue channel\n```\n\n---\n\n#### **Concatenating Augmented Data (If Applicable)**\n\nIf augmentation is enabled, the augmented data is concatenated to the original batch data:\n\n```python\nif augment:\n    batch_data = np.concatenate([batch_data, batch_data_aug])\n    batch_labels = np.concatenate([batch_labels, batch_labels])\n```\n\nThis allows the model to train on both the original and augmented data, increasing the variety of training examples.\n\n---\n\n#### **Yielding Data**\n\nFinally, the function yields the batch data and labels to be used by the training process.\n\n```python\nyield batch_data, batch_labels\n```\n\n- **`yield`**: This keyword returns the batch data and labels to the training process, allowing it to be used in the model’s training loop.\n\n---\n\n#### **Handling Remaining Data**\n\nAfter processing the full batches, any remaining data (i.e., data that doesn't fit evenly into full batches) is handled in a similar manner.\n\n```python\nif len(t) % batch_size != 0:\n    batch_data = np.zeros((len(t) % batch_size, x, y, z, 3))\n    batch_labels = np.zeros((len(t) % batch_size, 5))\n```\n\nThis ensures that even the remaining samples are processed in a batch, albeit potentially smaller than the specified batch size.\n\n---\n\n### Summary\n\n- **`generator_with_aug`** is a generator function that processes and augments image data in batches.\n- It allows for optional data augmentation (shifting, cropping, resizing).\n- The function normalizes image pixel values and prepares both original and augmented data for training.\n- The generator yields batches of data and labels, which can be used by machine learning models for training, especially in scenarios where the dataset is too large to fit into memory.\n\nThis method is highly useful when working with large video datasets and when real-time data augmentation is needed to improve model robustness.","metadata":{"id":"jyKqaCLbu5MI"}},{"cell_type":"code","source":"def generator_with_aug(source_path, folder_list, batch_size, y, z, normalise, augment=False):\n    # Print details about the source path, batch size, and target image resolution\n    print('Source path = ', source_path, '; batch size =', batch_size, '; Image resolution = ({},{})'.format(y, z))\n\n    # List of image indices to be used from each video (specific frames)\n    img_idx = [0, 1, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 27, 28, 29]\n    x = len(img_idx)  # Total number of images per video\n\n    # Infinite loop to yield batches continuously\n    while True:\n        # Shuffle the folder list to ensure randomness in data selection\n        t = np.random.permutation(folder_list)\n\n        # Calculate the number of batches for this set of data\n        num_batches = int(len(t) / batch_size)\n\n        # Iterate over each batch\n        for batch in range(num_batches):\n            # Initialize arrays to hold the data for the batch\n            batch_data = np.zeros((batch_size, x, y, z, 3))  # 3 channels (RGB)\n            batch_labels = np.zeros((batch_size, 5))  # One-hot encoding of labels (5 classes)\n\n            # Initialize augmented data array if augmentation is enabled\n            if augment:\n                batch_data_aug = np.zeros((batch_size, x, y, z, 3))  # Array to store augmented data\n\n            # Process each folder in the current batch\n            for folder in range(batch_size):\n                # List all images in the current folder\n                imgs = os.listdir(source_path + '/' + t[folder + (batch * batch_size)].split(';')[0])\n\n                # Iterate over selected image indices\n                for idx, item in enumerate(img_idx):\n                    # Load the image from the corresponding folder\n                    image = Image.open(source_path + '/' + t[folder + (batch * batch_size)].strip().split(';')[0] + '/' + imgs[item])\n\n                    # Apply data augmentation if enabled\n                    if augment:\n                        im_aug = image_augment(image, y, z)\n\n                    # Handle image cropping (if necessary) and resizing\n                    if image.size[0] == 160:\n                        # Crop a 120x120 region from the image\n                        image = image.crop((20, 0, 140, 120))  # left=20, top=0, right=140, bottom=120\n\n                    # Resize the image to the target resolution (y, z)\n                    image = image.resize(size=(y, z))\n                    image = np.asarray(image).astype(np.float32)\n\n                    # Normalize each color channel (RGB) separately\n                    batch_data[folder, idx, :, :, 0] = normalise(image[:, :, 0])  # Red channel\n                    batch_data[folder, idx, :, :, 1] = normalise(image[:, :, 1])  # Green channel\n                    batch_data[folder, idx, :, :, 2] = normalise(image[:, :, 2])  # Blue channel\n\n                    # If augmentation is enabled, normalize and store augmented images as well\n                    if augment:\n                        batch_data_aug[folder, idx, :, :, 0] = normalise(im_aug[:, :, 0])\n                        batch_data_aug[folder, idx, :, :, 1] = normalise(im_aug[:, :, 1])\n                        batch_data_aug[folder, idx, :, :, 2] = normalise(im_aug[:, :, 2])\n\n                # Set the label for the current folder (one-hot encoded label)\n                batch_labels[folder, int(t[folder + (batch * batch_size)].strip().split(';')[2])] = 1\n\n            # If augmentation is enabled, concatenate the original and augmented data\n            if augment:\n                batch_data = np.concatenate([batch_data, batch_data_aug])\n                batch_labels = np.concatenate([batch_labels, batch_labels])\n\n            # Yield the batch data and labels to be used by the model during training\n            yield batch_data, batch_labels\n\n        # Handle any remaining data that doesn't fit into full batches\n        if len(t) % batch_size != 0:\n            batch_data = np.zeros((len(t) % batch_size, x, y, z, 3))\n            batch_labels = np.zeros((len(t) % batch_size, 5))\n\n            if augment:\n                batch_data_aug = np.zeros((len(t) % batch_size, x, y, z, 3))  # Augmented data storage\n\n            # Process remaining folders\n            for folder in range(len(t) % batch_size):\n                imgs = os.listdir(source_path + '/' + t[folder + (num_batches * batch_size)].split(';')[0])\n\n                for idx, item in enumerate(img_idx):\n                    image = Image.open(source_path + '/' + t[folder + (num_batches * batch_size)].strip().split(';')[0] + '/' + imgs[item])\n\n                    if augment:\n                        im_aug = image_augment(image, y, z)\n\n                    # Crop and resize the image if necessary\n                    if image.size[0] == 160:\n                        image = image.crop((20, 0, 140, 120))\n\n                    image = image.resize(size=(y, z))\n                    image = np.asarray(image).astype(np.float32)\n\n                    # Normalize the image channels\n                    batch_data[folder, idx, :, :, 0] = normalise(image[:, :, 0])\n                    batch_data[folder, idx, :, :, 1] = normalise(image[:, :, 1])\n                    batch_data[folder, idx, :, :, 2] = normalise(image[:, :, 2])\n\n                    # Normalize and store augmented data if augmentation is enabled\n                    if augment:\n                        batch_data_aug[folder, idx, :, :, 0] = normalise(im_aug[:, :, 0])\n                        batch_data_aug[folder, idx, :, :, 1] = normalise(im_aug[:, :, 1])\n                        batch_data_aug[folder, idx, :, :, 2] = normalise(im_aug[:, :, 2])\n\n                # Set the label for the current folder\n                batch_labels[folder, int(t[folder + (num_batches * batch_size)].strip().split(';')[2])] = 1\n\n            # If augmentation is enabled, concatenate augmented data with the original data\n            if augment:\n                batch_data = np.concatenate([batch_data, batch_data_aug])\n                batch_labels = np.concatenate([batch_labels, batch_labels])\n\n            # Yield the final batch of remaining data\n            yield batch_data, batch_labels","metadata":{"id":"mKISmSXejcTw"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Conv3D Model Architecture with Data Augmentation\nThe Conv3D model architecture with data augmentation is designed for analyzing volumetric data, such as videos or 3D medical scans, by leveraging three-dimensional convolutional layers. The model architecture typically consists of several 3D convolutional layers followed by activation functions like ReLU and max-pooling layers to reduce the spatial dimensions of the input data while preserving essential features. This architecture allows the model to capture spatial patterns and temporal dependencies across multiple frames in the case of video data or within a 3D volume in medical imaging tasks.\n\nThe model typically starts with a series of Conv3D layers, where each layer learns features from its input by applying convolution operations with 3D filters. After each convolutional operation, a max-pooling layer is usually applied to downsample the feature maps and extract more abstract representations. These convolutional and pooling layers are followed by dense layers to make final predictions based on the learned features.\n\nTo enhance the model's ability to generalize, data augmentation is applied. This includes random transformations such as rotations, shifts, and flips, which artificially increase the size and variability of the dataset, helping to prevent overfitting. The combined use of Conv3D layers and data augmentation helps in building robust models capable of handling complex 3D data across various domains like action recognition and medical imaging.","metadata":{"id":"FwqxvIH0C3h9"}},{"cell_type":"markdown","source":"## Creating Conv3dArchitecture with Data Augmentation\n\nThe Conv3D architecture for this project utilizes 3D convolutional layers to process volumetric data, making it suitable for tasks such as action recognition or medical imaging. To improve model performance and generalization, data augmentation techniques such as rotation, flipping, and scaling are applied to the input data. These techniques artificially expand the training dataset, reducing the risk of overfitting and helping the model learn more robust features. The Conv3D layers capture spatial and temporal patterns in 3D data, enabling the model to learn both local and global representations effectively. This approach ensures better accuracy and model robustness during training and validation.","metadata":{"id":"6drOivX9AkqC"}},{"cell_type":"code","source":"def Conv3dArchitecture(x, y, z):\n    # Initialize a Sequential model for building the Conv3D architecture\n    conv3d_model = Sequential()\n\n    # First Conv3D Layer:\n    # 64 filters, kernel size of (3,3,3), 'same' padding, input shape (x, y, z, 3) for RGB images\n    conv3d_model.add(Conv3D(64, (3, 3, 3), strides=(1, 1, 1), padding='same', input_shape=(x, y, z, 3)))\n    conv3d_model.add(BatchNormalization())  # Apply Batch Normalization to stabilize training\n    conv3d_model.add(Activation('relu'))  # Use ReLU activation function\n    conv3d_model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2)))  # Apply max pooling with stride of (2,2,2)\n\n    # Second Conv3D Layer:\n    # 128 filters, kernel size of (3,3,3), 'same' padding\n    conv3d_model.add(Conv3D(128, (3, 3, 3), strides=(1, 1, 1), padding='same'))\n    conv3d_model.add(BatchNormalization())  # Apply Batch Normalization\n    conv3d_model.add(Activation('relu'))  # Use ReLU activation function\n    conv3d_model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2)))  # Apply max pooling with stride of (2,2,2)\n\n    # Third Conv3D Layer:\n    # 256 filters, kernel size of (3,3,3), 'same' padding\n    conv3d_model.add(Conv3D(256, (3, 3, 3), strides=(1, 1, 1), padding='same'))\n    conv3d_model.add(BatchNormalization())  # Apply Batch Normalization\n    conv3d_model.add(Activation('relu'))  # Use ReLU activation function\n    conv3d_model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2)))  # Apply max pooling with stride of (2,2,2)\n\n    # Fourth Conv3D Layer:\n    # Another set of 256 filters with 'same' padding\n    conv3d_model.add(Conv3D(256, (3, 3, 3), strides=(1, 1, 1), padding='same'))\n    conv3d_model.add(BatchNormalization())  # Apply Batch Normalization\n    conv3d_model.add(Activation('relu'))  # Use ReLU activation function\n    conv3d_model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2)))  # Apply max pooling with stride of (2,2,2)\n\n    # Flattening the output to connect to fully connected (dense) layers\n    conv3d_model.add(Flatten())  # Flatten the 3D output to 1D\n\n    # Dropout Layer:\n    # Dropout to avoid overfitting (50% of neurons will be randomly dropped during training)\n    conv3d_model.add(Dropout(0.5))\n\n    # Fully connected layer with 512 units and ReLU activation\n    conv3d_model.add(Dense(512, activation='relu'))\n\n    # Dropout layer again to avoid overfitting\n    conv3d_model.add(Dropout(0.5))\n\n    # Final output layer with 5 units for 5 classes, using softmax activation for multi-class classification\n    conv3d_model.add(Dense(5, activation='softmax'))\n\n    # Return the constructed model\n    return conv3d_model","metadata":{"id":"fTjBlUyz80zU"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Summary**\n\nThis model is a 3D Convolutional Neural Network (Conv3D) designed for video or volumetric data classification. It extracts spatial features across three dimensions and classifies the input into one of five categories.\n\n1. **Model Initialization**:\n   - A `Sequential` model is created to stack layers in sequence, starting with the Conv3D layers.\n\n2. **Conv3D Layers**:\n   - Conv3D layers are added to the model sequentially, starting with 64 filters and doubling the number of filters with each subsequent layer.\n   - Each Conv3D layer uses a kernel size of (3, 3, 3) and applies 'same' padding to maintain the spatial dimensions of the input.\n   - Each Conv3D layer is followed by batch normalization and ReLU activation to increase the network's ability to learn complex patterns.\n\n3. **MaxPooling3D Layers**:\n   - After each Conv3D layer, a MaxPooling3D layer is applied to downsample the feature maps, reducing their spatial size by half with a pool size and stride of (2, 2, 2).\n\n4. **Flattening**:\n   - The output of the last Conv3D layer is 3D, so a `Flatten` layer is used to convert it into a 1D vector to connect to the fully connected (Dense) layers.\n\n5. **Fully Connected (Dense) Layers**:\n   - A Dense layer with 512 units and ReLU activation is used to connect the flattened output to the final output layer.\n   - Dropout (50%) is applied after the fully connected layers to prevent overfitting during training.\n\n6. **Output Layer**:\n   - The final layer is a Dense layer with 5 units, corresponding to the number of classes in the classification problem. The softmax activation function is used to output probabilities for each class.","metadata":{"id":"GUUTEHcvu5MJ"}},{"cell_type":"markdown","source":"### Evaluating Model No. 6\n- **Model**: Model 6 - Conv3D Archtecture with Data Augmentation\n- **Batch Size**: 20\n- **Number of Epochs**: 20\n- **Image Resolution**: 100x100\n- **Normalization Method**: Pixel values divided by 255 (scaling between 0 and 1)","metadata":{"id":"Os1ZjJQe8vnR"}},{"cell_type":"code","source":"# Import necessary libraries for date and time operations\nimport datetime\n\n# Set the current time for creating unique filenames for checkpoints\ncurr_dt_time = datetime.datetime.now()\n\n# Set the number of epochs and batch size for model training\nnum_epochs = 20  # Number of epochs for training the model\nbatch_size = 20  # Batch size determines how many samples are processed before the model is updated\n\n# Set the number of training and validation sequences in the dataset\n# This assumes that `train_doc` and `val_doc` are lists containing information about the sequences\nnum_train_sequences = len(train_doc)  # Total number of training sequences\nnum_val_sequences = len(val_doc)  # Total number of validation sequences\n\n# Define the indices of images to use in each video sequence for training\n# This allows the model to select specific frames from a sequence for processing\nimg_idx = [0, 1, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 27, 28, 29]\nx = len(img_idx)  # Number of images used in each sequence (based on the indices above)\n\n# Set the desired image resolution for the input images (height and width)\ny = 100  # Height of the image (100 pixels)\nz = 100  # Width of the image (100 pixels)","metadata":{"id":"-uXPYh1b802d"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Compile the Conv3D Model using SGD Optimizer**\n1. **SGD Optimizer**:\n   - We define the SGD (Stochastic Gradient Descent) optimizer to update model weights during training.\n   - The hyperparameters for the optimizer are:\n     - `learning_rate`: Controls the size of the steps taken in the direction of the gradient (0.001).\n     - `decay`: L2 regularization term to prevent overfitting (1e-6).\n     - `momentum`: Accelerates the learning process by adding a fraction of the previous weight update (set to 0.7).\n     - `nesterov`: Uses Nesterov momentum to provide a more informed update for faster convergence.\n\n2. **Conv3D Model Initialization**:\n   - The `Conv3dArchitecture(x, y, z)` function is called to create the model with the specified input dimensions: `x` (number of frames), `y` (height of the image), and `z` (width of the image).\n\n3. **Model Compilation**:\n   - The `compile()` function configures the model for training:\n     - We use the `sgd` optimizer defined earlier.\n     - The loss function is set to `categorical_crossentropy`, which is commonly used for multi-class classification problems.\n     - The model tracks `categorical_accuracy` as the metric, meaning it will evaluate the proportion of correctly classified samples during training.\n\n4. **Model Summary**:\n   - Finally, the `summary()` function is called to display the model's architecture, including the layers, output shapes, and the number of parameters at each layer. This helps in verifying the model's structure before training.","metadata":{"id":"CMa8Hq5Z806K"}},{"cell_type":"code","source":"# Compile the Conv3D model\n# Here we are using the SGD (Stochastic Gradient Descent) optimizer for training\n\n# Define the SGD optimizer with specific hyperparameters:\n# - learning_rate: Initial learning rate for optimization (0.001)\n# - decay: The weight decay (regularization) factor to help prevent overfitting (1e-6)\n# - momentum: Momentum parameter to accelerate convergence (set to 0.7)\n# - nesterov: Boolean indicating whether to use Nesterov accelerated gradients (set to True for faster convergence)\nsgd = optimizers.SGD(learning_rate=0.001, decay=1e-6, momentum=0.7, nesterov=True)\n\n# Initialize the Conv3D model using the previously defined architecture function\n# `x`, `y`, `z` are the dimensions of the input data (number of frames, height, width)\nconv3d_model6 = Conv3dArchitecture(x, y, z)\n\n# Compile the model with:\n# - `optimizer`: The optimizer chosen earlier (SGD)\n# - `loss`: Categorical cross-entropy loss function (used for multi-class classification)\n# - `metrics`: We are tracking categorical accuracy during training\nconv3d_model6.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n\n# Print the model summary to view the architecture, layer configurations, and parameters\nprint(conv3d_model6.summary())","metadata":{"id":"KJu1fhd5BQrc","outputId":"8c8773e7-8d32-41a5-b480-9653cf4c3ec9"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Set up and train the Conv3D model with data augmentation and validation**\n\n1. **Callbacks (`callbacks_list`)**:  \n   - The `callback_directory` function is called to generate the file paths for saving model checkpoints and logs during training.\n\n2. **Steps Per Epoch**:  \n   - The `steps_per_epoch` variable calculates how many steps (or batches) will be executed in one epoch. It's based on the number of training sequences divided by the batch size.\n   - If there is a remainder when dividing `num_train_sequences` by `batch_size`, we add 1 to ensure all data is used.\n\n3. **Validation Steps**:  \n   - Similarly, the `validation_steps` variable calculates how many validation steps will be executed in one epoch based on the number of validation sequences and the batch size.\n   - If there is a remainder when dividing `num_val_sequences` by `batch_size`, we add 1.\n\n4. **Normalization**:  \n   - A lambda function `normalise` is defined to scale the image pixel values from the range `[0, 255]` to `[0, 1]`. This is important for neural network training, as scaled values improve convergence.\n\n5. **Data Generators (`train_generator` and `val_generator`)**:  \n   - The `train_generator` is responsible for yielding batches of augmented training data using the `generator_with_aug` function. It applies data augmentation if `augment=True`.\n   - The `val_generator` is used for the validation data but without augmentation.\n\n6. **Model Training (`fit`)**:  \n   - The `fit()` method is used to train the model. It takes the following arguments:\n     - `train_generator`: The training data generator.\n     - `steps_per_epoch`: The number of steps (batches) to run per epoch.\n     - `epochs`: The total number of epochs to train the model.\n     - `verbose`: Level of output verbosity (1 means progress bars).\n     - `callbacks`: List of callbacks to apply during training.\n     - `validation_data`: The validation data generator.\n     - `validation_steps`: The number of steps to run per epoch for validation.\n     - `class_weight`: Optionally assigns weights to the classes (set to `None` here).\n     - `workers`: The number of worker threads to load data in parallel.\n\nThis setup ensures the model is trained efficiently with the appropriate batch and validation steps while also applying data augmentation during training to improve generalization.","metadata":{"id":"WfLXzgpb9bOS"}},{"cell_type":"code","source":"# Call the function to create the file path for saving model checkpoints and callbacks\ncallbacks_list = callback_directory(curr_dt_time)\n\n# Assign the number of steps per epoch for training and validation datasets\n# The generator will create batch_size * 2 data for each batch (raw data + augmented data)\n# However, the number of steps per epoch remains the same. The total number of samples per batch increases due to augmentation.\n\n# Calculate steps per epoch for the training data:\n# If the total number of training sequences is divisible by the batch size, we calculate steps per epoch by dividing the number of sequences by the batch size.\n# Otherwise, we round up by adding 1 to ensure all data is used.\nif (num_train_sequences % batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences / batch_size)\nelse:\n    steps_per_epoch = (num_train_sequences // batch_size) + 1\n\n# Calculate validation steps similarly to ensure all validation data is processed.\n# The batch size for validation is the same as for training, but the validation steps may differ based on the number of sequences.\nif (num_val_sequences % batch_size) == 0:\n    validation_steps = int(num_val_sequences / batch_size)\nelse:\n    validation_steps = (num_val_sequences // batch_size) + 1\n\n# Define the normalization function that scales pixel values to [0, 1] range.\nnormalise = lambda a: a / 255.0\n\n# Create the train_generator and val_generator using the `generator_with_aug` function.\n# These generators will provide the data in batches for training and validation, with optional augmentation for the training data.\ntrain_generator = generator_with_aug(train_path, train_doc, batch_size, y, z, normalise, augment=True)\nval_generator = generator_with_aug(val_path, val_doc, batch_size, y, z, normalise)\n\n# Train the model using the `fit` function. This function will train the model for a number of epochs.\nhistory_model6 = conv3d_model6.fit(\n    train_generator,                 # Training data generator\n    steps_per_epoch=steps_per_epoch, # Number of steps per epoch during training\n    epochs=num_epochs,              # Total number of training epochs\n    verbose=1,                       # Display progress during training (1 means verbose output)\n    callbacks=callbacks_list,        # List of callbacks to monitor during training (e.g., saving checkpoints)\n    validation_data=val_generator,  # Validation data generator\n    validation_steps=validation_steps, # Number of steps per epoch during validation\n    class_weight=None,              # Weights for the classes (None indicates no weighting)\n    workers=1,                      # Number of workers to load data in parallel\n    initial_epoch=0                 # Starting epoch (usually set to 0 for new training)\n)","metadata":{"id":"itnGPhzuBXgQ","outputId":"9a345dca-c8c3-4694-9f86-b76803b35e74"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print_model_metrics(\"Model No. 6 - Based on Conv3D Architecture with Data Augmentation\", history_model6)","metadata":{"id":"gU4VbCyTu5ML","outputId":"9de8c775-40b8-4f47-ae82-423912b6002d"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualizing Model 6\nVisualizing the model's performance through plots of training and validation accuracy and loss across epochs provides valuable insights into the learning process. These visualizations help evaluate how well the model is learning from the data and generalizing to unseen examples. By examining the trends, we can identify potential issues like overfitting or underfitting. Overfitting is indicated when the model performs well on training data but poorly on validation data, while underfitting is when the model fails to capture patterns in both datasets. This information allows for effective hyperparameter tuning to improve model performance and generalization.","metadata":{"id":"ycCARmhdu5ML"}},{"cell_type":"code","source":"plot_history(history_model6, num_epochs)","metadata":{"id":"MHJ91kdCClYF","outputId":"351ee9ff-34d9-45c2-d2fe-84a2d0efd700"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model No. 6 - Observations\n\n- **Training Accuracy**: Gradually improved from 33.03% to 60.86% by epoch 20.\n- **Validation Accuracy**: Increased steadily from 16.00% to 59.00%.\n- **Loss**: Training loss decreased significantly, from 2.55 to 0.96 by epoch 20. Validation loss showed consistent improvement, reducing from 1.66 to 1.05.\n- **Learning Rate**: Reduced progressively at various epochs using `ReduceLROnPlateau`, starting from 0.001 and decreasing to 1e-05.\n- **Model Saving**: The model was saved after each epoch, with notable improvements in both training and validation accuracy and loss.\n\n**Overall Summary - It's a Good Model**\n\nThe model exhibited steady improvement across both training and validation metrics over the course of 20 epochs. Training accuracy rose from 33.03% to 60.86%, while validation accuracy improved from 16.00% to 59.00%. Training loss decreased significantly, while validation loss also showed a continuous reduction. The learning rate was dynamically adjusted using `ReduceLROnPlateau`, further optimizing performance. The model was saved after each epoch, showcasing consistent improvements in training and validation results throughout the training process.","metadata":{"id":"3Y1ODOE-91Pc"}},{"cell_type":"markdown","source":"# CNN, LSTM and GRU with Transfer Learning and Data Augmentation\n\n- **Convolutional Neural Networks (CNN)**:\n  - Specialized for processing grid-like data, such as images or 3D volumes.\n  - Uses convolutional layers to extract local features and pooling layers to reduce spatial dimensions.\n  - Ideal for feature extraction from raw data, which can be further analyzed by deeper layers.\n\n- **Long Short-Term Memory (LSTM)**:\n  - A type of Recurrent Neural Network (RNN) designed to capture long-term dependencies in sequential data.\n  - Effective for tasks like time-series prediction, language modeling, and action recognition.\n  - LSTM units store relevant information over longer periods, mitigating vanishing gradient issues.\n\n- **Gated Recurrent Units (GRU)**:\n  - A simplified version of LSTM with fewer parameters and similar performance in many cases.\n  - GRUs use gates to control the flow of information, making them efficient for sequential data tasks.\n  - Often preferred for tasks where training time is critical.\n\n- **Transfer Learning**:\n  - Involves using pre-trained models on large datasets and fine-tuning them on smaller, domain-specific datasets.\n  - Improves performance and reduces training time, as models have already learned relevant features from large datasets.\n\n- **Data Augmentation**:\n  - Involves applying random transformations like rotations, flips, and shifts to artificially expand the dataset.\n  - Helps prevent overfitting by exposing the model to more varied input data, improving generalization.","metadata":{"id":"fi1cJy3YD0WO"}},{"cell_type":"markdown","source":"## Creating CNN, LSTM and GRU with Transfer Learning and Data Augmentation\n\nThe model combines CNN for feature extraction, LSTM/GRU for sequential data processing, with Transfer Learning to leverage pre-trained models and fine-tune them for specific tasks. Data Augmentation is applied to enhance training by generating diverse inputs, improving generalization and model robustness, especially for tasks like time-series and image analysis.","metadata":{"id":"G0qrb4rSEJFC"}},{"cell_type":"code","source":"# Define CNN-LSTM-GRU model architecture with VGG16 as the base for feature extraction\n\ndef cnn_lstm_gru(x, y, z, dropout):\n    # Load the pre-trained VGG16 model without the top (fully connected) layers\n    # Using 'imagenet' weights, and specifying the input shape as (y, z, 3)\n    base_model = VGG16(include_top=False, weights='imagenet', input_shape=(y, z, 3))\n\n    # Flatten the output of the VGG16 base model\n    a = base_model.output\n    a = Flatten()(a)  # Flattening the features to a 1D vector\n    features = Dense(64, activation='relu')(a)  # Adding a Dense layer with 64 units\n\n    # Create a new model for feature extraction using the base VGG16 model\n    conv_model = Model(inputs=base_model.input, outputs=features)\n\n    # Freeze the layers of the base VGG16 model so that they are not trainable\n    for layer in base_model.layers:\n        layer.trainable = False\n\n    # Define the main sequential model\n    model = Sequential()\n\n    # Add the TimeDistributed wrapper to apply the convolutional model to each time step in the input sequence\n    model.add(TimeDistributed(conv_model, input_shape=(x, y, z, 3)))\n\n    # Add an LSTM layer with 128 units, return sequences for the next layer\n    model.add(LSTM(128, return_sequences=True))\n\n    # Apply dropout to prevent overfitting\n    model.add(Dropout(dropout))\n\n    # Add a GRU layer with 64 units, again returning sequences\n    model.add(GRU(64, return_sequences=True))\n\n    # Apply another dropout layer\n    model.add(Dropout(dropout))\n\n    # Add another GRU layer with 32 units, no return sequences since it's the last recurrent layer\n    model.add(GRU(32))\n\n    # Add a Dense layer with 64 units for further feature learning\n    model.add(Dense(64, activation='relu'))\n\n    # Add a dropout layer for regularization\n    model.add(Dropout(dropout))\n\n    # Output layer with 5 units (assuming 5 classes) and softmax activation for classification\n    model.add(Dense(5, activation='softmax'))\n\n    return model","metadata":{"id":"y1_scjy5jxMy"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Evaluating Model No. 7\n- **Model**: Model 7 - CNN, LSTM and GRU with Transfer learning\n- **Batch Size**: 32\n- **Number of Epochs**: 30\n- **Image Resolution**: 100x100\n- **Optimizer**: Adam\n- **Normalization Method**: Pixel values divided by 255 (scaling between 0 and 1)\n\n**Note:** We will now **implement data augmentation on Model No. 5**, which has proven to be our best-performing model to date.","metadata":{"id":"YFMjNvblkA1u"}},{"cell_type":"code","source":"# Set up unique checkpoint filenames based on current time\ncurr_dt_time = datetime.datetime.now()\n\n# Define the number of epochs and batch size for training\nnum_epochs = 30  # Total number of training epochs\nbatch_size = 32  # Number of samples per batch\n\n# Define the indices for the images in the sequence, selecting specific frames\nimg_idx = [0, 1, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 27, 28, 29]\nx = len(img_idx)  # The total number of images to be used per video sequence\ny = 100  # Image height for resizing\nz = 100  # Image width for resizing\n\n# Calculate the total number of training and validation sequences (i.e., video clips)\nnum_train_sequences = len(train_doc)  # Number of training sequences\nnum_val_sequences = len(val_doc)  # Number of validation sequences\n\n# Calculate the number of steps per epoch for training and validation\n# Steps per epoch depend on the total number of sequences divided by the batch size\n# If there is a remainder, we add one additional step for the remaining samples\n\n# Steps per epoch for training dataset\nif (num_train_sequences % batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences / batch_size)  # Exact division, no remainder\nelse:\n    steps_per_epoch = (num_train_sequences // batch_size) + 1  # Add extra step for leftover samples\n\n# Steps per epoch for validation dataset\nif (num_val_sequences % batch_size) == 0:\n    validation_steps = int(num_val_sequences / batch_size)  # Exact division, no remainder\nelse:\n    validation_steps = (num_val_sequences // batch_size) + 1  # Add extra step for leftover samples","metadata":{"id":"H4pRNN4PrVyC"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compile the model using the Adam optimizer\n# Adam optimizer is used here with a small learning rate of 0.0001 for better convergence\n\n# Create an Adam optimizer with a learning rate of 0.0001\nadam = optimizers.Adam(learning_rate=0.0001)\n\n# Initialize the model by calling the cnn_lstm_gru function\n# Pass the image dimensions (x, y, z) and dropout rate (0.25) to the model\nmodel7 = cnn_lstm_gru(x, y, z, 0.25)\n\n# Compile the model with the Adam optimizer\n# We use categorical cross-entropy as the loss function since it's a multi-class classification problem\n# 'categorical_accuracy' is used as the metric to evaluate the accuracy of the model during training\nmodel7.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n\n# Print the model summary to get an overview of the model architecture\nprint(model7.summary())","metadata":{"id":"8jk91c_HrV6h","outputId":"24e78256-c644-4c71-b66c-b9d62138f563"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Call the function to create a callback directory for saving model checkpoints\n# The 'callback_directory' function will create a directory based on the current date and time,\n# and return a list of callback functions to be used during training\ncallbacks_list = callback_directory(curr_dt_time)\n\n# Create the data generators for training and validation\n# The 'train_generator' and 'val_generator' will be used in the model's .fit_generator method\n# These generators will load and augment images during training and validation\n\n# Define a normalization function to scale pixel values to the range [0, 1]\nnormalise = lambda a: a / 255.0\n\n# Create the training data generator with augmentation enabled\n# The 'True' flag for augmentation indicates that data augmentation (e.g., random transformations) will be applied to the training data\ntrain_generator = generator_with_aug(train_path, train_doc, batch_size, y, z, normalise, augment=True)\n\n# Create the validation data generator without augmentation (only resizing and normalizing the images)\nval_generator = generator_with_aug(val_path, val_doc, batch_size, y, z, normalise)\n","metadata":{"id":"6OAzMz_zrV-J"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fit the model on the training data\n# The model will be trained using the .fit method, with the following parameters:\n\nhistory_model7 = model7.fit(\n    # The training data generator that provides batches of data for training\n    train_generator,\n\n    # The number of steps per epoch, which is the number of batches of data to process in each epoch\n    steps_per_epoch=steps_per_epoch,\n\n    # The number of epochs to train the model\n    epochs=num_epochs,\n\n    # Verbosity level of the training process (1 will show progress bar)\n    verbose=1,\n\n    # List of callbacks to apply during training (e.g., saving model checkpoints, early stopping)\n    callbacks=callbacks_list,\n\n    # The validation data generator that provides batches of data for validation\n    validation_data=val_generator,\n\n    # The number of steps per validation epoch\n    validation_steps=validation_steps,\n\n    # Class weights can be provided to handle class imbalance (None means no class weights)\n    class_weight=None,\n\n    # The number of workers for data loading in parallel\n    workers=1,\n\n    # The initial epoch from which to start training (0 means start from the beginning)\n    initial_epoch=0\n)","metadata":{"id":"M4WMAQ78rWBe","outputId":"28c7871d-e545-4932-a7d6-49608ca31cc3"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print_model_metrics(\"Model No. 7 - CNN, LSTM and GRU with Transfer Learning and Data Augmentation\", history_model7)","metadata":{"id":"NjRb2NJJu5MO","outputId":"2baebc94-cdb8-4bef-b782-83409f7f560d"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualizing Model No. 7\n- Visualizing model performance by plotting training and validation accuracy and loss over epochs.\n- Helps evaluate how well the model learns from the data and generalizes to new data.\n- Identifies potential issues like overfitting (good performance on training, poor on validation) or underfitting (poor performance on both).\n- Enables effective tuning of hyperparameters to improve model performance and generalization.\n- Provides valuable insights to optimize the learning process and adjust the model for better results.","metadata":{"id":"lQs4fZ9cu5MP"}},{"cell_type":"code","source":"plot_history(history_model7, num_epochs)","metadata":{"id":"Da0IPiPfrWFX","outputId":"2bdef227-a0e5-41d3-e575-26f69f59b0db"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model No. 7 - Observations\n\n- **Training Accuracy**: Increased consistently throughout the epochs, reaching 98.87% by epoch 30.\n- **Validation Accuracy**: Showed fluctuations, peaking at 75.00% in epoch 18.\n- **Loss**: Training loss steadily decreased, while validation loss showed fluctuations but improved overall during the first 20 epochs before stabilizing.\n- **Learning Rate**: Reduced multiple times using the ReduceLROnPlateau callback, lowering from 0.0001 to 0.00001.\n- **Model Saving**: Model was saved after every epoch with significant improvement in both training and validation accuracy.\n\n**Overall Summary**\n\nThe model showed steady improvement in training accuracy, achieving a remarkable 98.87% by epoch 30. The validation accuracy fluctuated, with the highest point at 75.00% in epoch 18, suggesting the model might have benefited from further optimization. Training loss demonstrated consistent decrease, indicating the model was learning effectively. However, the validation loss fluctuated, particularly after the initial improvements. The learning rate was adjusted multiple times using the ReduceLROnPlateau callback, which helped maintain the model's performance over the epochs. Despite some variability in the validation accuracy, the model's overall progress in training accuracy and loss reduction indicates it was successfully adapting and learning, with checkpoints being saved regularly, reflecting significant milestones in its training process.","metadata":{"id":"cKJn2SRp56kA"}},{"cell_type":"markdown","source":"# Transfer Learning using MobileNet and GRU\n\nTransfer learning is a technique that allows the reuse of a pre-trained model for a new task, significantly reducing the time and data needed for training. In the case of using MobileNet and GRU, transfer learning leverages the power of MobileNet’s pre-trained convolutional layers for feature extraction, combined with the sequential learning abilities of GRU (Gated Recurrent Unit) for time-series or sequence-based tasks.\n\nMobileNet is a lightweight deep learning model designed for mobile and embedded devices. It is pre-trained on large-scale datasets like ImageNet, enabling it to learn rich image features. By using the pre-trained MobileNet model, the model can extract features from the input images, which significantly improves accuracy, especially when data is limited.\n\nGRU is a type of recurrent neural network (RNN) used for handling sequential data. It works well for tasks that involve time-series data or sequences, as it captures dependencies across time steps without the vanishing gradient problem that affects traditional RNNs. When combined with MobileNet, GRU processes the sequential features extracted from images or frames, making it ideal for applications like video classification or sequential image data analysis.\n\nThis combined architecture—MobileNet for feature extraction and GRU for sequence modeling—offers a powerful, efficient solution for tasks like action recognition, video processing, and other time-dependent tasks, leveraging both transfer learning and advanced sequence modeling.","metadata":{"id":"wEXht1g4GVyS"}},{"cell_type":"code","source":"# Load the MobileNet model with pre-trained weights from ImageNet\nmobilenet_transfer = mobilenet.MobileNet(weights='imagenet', include_top=False)","metadata":{"id":"1vklLFZFBvCY","outputId":"cf6bcb09-b620-445d-ea6b-50b27e05f229"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1. **MobileNet**:\n   - MobileNet is a deep learning model architecture designed for efficient performance on mobile and embedded devices. It is known for its lightness and speed compared to other deep learning models, making it ideal for real-time applications.\n\n2. **weights='imagenet'**:\n   - This parameter specifies that the model should be loaded with weights pre-trained on the ImageNet dataset. ImageNet is a large visual database used for training models in object recognition tasks. Using pre-trained weights allows the model to leverage learned features, improving performance on a new task, especially when the available dataset is small.\n   \n3. **include_top=False**:\n   - The `include_top=False` parameter tells the model to exclude the fully connected layers (the \"top\" layers) that are typically used for classification in ImageNet (i.e., the final dense layer that produces the 1000 ImageNet classes). This is useful when you want to use the pre-trained convolutional layers as feature extractors for a different task, without the need for ImageNet-specific classification layers.\n   - By setting `include_top=False`, you retain the convolutional layers of MobileNet, which can be used to extract features from images, while allowing you to add your own custom classification or regression layers on top as needed.\n\n**Purpose**\n\nThis line of code initializes the MobileNet model with pre-trained weights from ImageNet but excludes the final classification layers, allowing you to use MobileNet's feature extraction capabilities for your own task.","metadata":{"id":"j8rhacVHu5MQ"}},{"cell_type":"markdown","source":"## Creating Transfer Learrning using Mobilenet and GRU Model\n\n- **MobileNet for Feature Extraction**: Utilizes pre-trained MobileNet, which extracts high-level features from images, benefiting from training on large datasets like ImageNet.\n- **Transfer Learning**: Leverages the knowledge from MobileNet’s pre-trained layers, reducing the need for extensive training data and speeding up the model training process.\n- **GRU for Sequence Modeling**: Integrates GRU, a type of recurrent neural network, for handling sequential or time-series data by capturing temporal dependencies.\n- **Efficient Architecture**: Combines MobileNet’s lightweight nature with GRU’s ability to model sequences, ensuring efficiency and performance.\n- **Applications**: Ideal for tasks like action recognition, video classification, and other sequence-based image analysis.","metadata":{"id":"KxOSUZpiBXrT"}},{"cell_type":"code","source":"def cnn_gru(x, y, z, dropout):\n    # Initialize a Sequential model\n    model = Sequential()\n\n    # Add a TimeDistributed wrapper around the MobileNet model for each time step in the sequence.\n    # This allows the model to process sequential data (e.g., video frames or image sequences)\n    # with MobileNet as the feature extractor for each frame.\n    model.add(TimeDistributed(mobilenet_transfer, input_shape=(x, y, z, 3)))\n\n    # Add TimeDistributed BatchNormalization to normalize the activations and gradients\n    # during training, improving convergence speed and overall performance.\n    model.add(TimeDistributed(BatchNormalization()))\n\n    # Apply MaxPooling2D layer for down-sampling the spatial dimensions of the feature maps.\n    # The pooling operation reduces the dimensionality and computational complexity.\n    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n\n    # Flatten the 2D feature maps into a 1D vector, making it suitable for feeding into the RNN layers.\n    model.add(TimeDistributed(Flatten()))\n\n    # Add a GRU layer to capture temporal dependencies in the sequence of features.\n    # GRU (Gated Recurrent Unit) is an RNN variant designed to capture sequential information\n    # more effectively than vanilla RNNs while maintaining efficiency.\n    model.add(GRU(128, kernel_regularizer=regularizers.l2(0.01)))  # L2 regularization\n\n    # Add Dropout regularization to reduce overfitting during training by randomly setting\n    # a fraction of input units to zero at each update during training time.\n    model.add(Dropout(dropout))\n\n    # Add a Dense layer with ReLU activation for learning high-level feature representations\n    # before the output layer.\n    model.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01)))  # L2 regularization\n\n    # Add Dropout again to prevent overfitting and improve model generalization.\n    model.add(Dropout(dropout))\n\n    # Final Dense layer with softmax activation for multi-class classification.\n    # This produces a probability distribution over 5 output classes.\n    model.add(Dense(5, activation='softmax'))\n\n    # Return the constructed model\n    return model","metadata":{"id":"CzKYe5OOBSBc"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1. **TimeDistributed Wrapper**:\n   - The `TimeDistributed` layer applies a given layer (e.g., MobileNet) independently to each time step of the input sequence. This is useful when working with sequential data, such as videos or image sequences, where each frame is treated individually but shares the same model structure.\n\n2. **MobileNet**:\n   - `mobilenet_transfer` is used as a feature extractor for each time step. It's a pre-trained MobileNet model without the final classification layers, and it's applied to each frame in the input sequence.\n\n3. **BatchNormalization**:\n   - Normalizes the output of the previous layer by scaling and shifting, which helps speed up training and improves the model's generalization.\n\n4. **MaxPooling2D**:\n   - Reduces the spatial dimensions (height and width) of the feature maps, which decreases computational complexity and helps extract dominant features.\n\n5. **Flatten**:\n   - Converts the 2D feature maps from the previous layers into a 1D vector, preparing the data for the next layer, which is a recurrent layer.\n\n6. **GRU Layer**:\n   - The Gated Recurrent Unit (GRU) layer is used to capture temporal dependencies from the sequence of extracted features. GRUs are a type of Recurrent Neural Network (RNN) that helps the model remember important information from previous time steps while avoiding the vanishing gradient problem.\n\n7. **Dense Layer**:\n   - The Dense layers with ReLU activation learn high-level features, followed by a softmax output layer that provides class probabilities.\n\n8. **Dropout**:\n   - Dropout is a regularization technique that helps prevent overfitting by randomly dropping a fraction of the input units during training.\n\n9. **Final Output Layer (Softmax)**:\n   - The softmax activation function in the output layer is used for multi-class classification, providing a probability distribution over the 5 possible classes.\n\n**Purpose**\n\nThis model is designed for sequential input data, such as video frames or time-series images, where MobileNet serves as the feature extractor for each frame, and the GRU layer captures temporal dependencies across the sequence of frames. It is a combination of a convolutional neural network (CNN) for feature extraction and a recurrent neural network (RNN) for modeling sequences. The output is a classification into 5 possible classes.","metadata":{"id":"3UV8-SoOu5MR"}},{"cell_type":"markdown","source":"### Evaluating Model No. 8\n- **Model**: Model 8 - Transfer Learrning using Mobilenet and GRU\n- **Batch Size**: 16\n- **Number of Epochs**: 30\n- **Image Resolution**: 100x100\n- **Optimizer**: Adam\n- **Normalization Method**: Pixel values divided by 255 (scaling between 0 and 1)","metadata":{"id":"tEcvNccVGVjq"}},{"cell_type":"code","source":"# Set the current date and time to create unique filenames for checkpoints\ncurr_dt_time = datetime.datetime.now()\n\n# Define the number of epochs for training\nnum_epochs = 30  # Set the number of epochs to train the model\n\n# Define batch size\nbatch_size = 16  # Set the batch size to 16\n\n# Define the list of indices for the images in the sequence to be used for each video frame\nimg_idx = [0, 1, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 27, 28, 29]\nx = len(img_idx)  # The number of images to use per video sequence\n\n# Define image resolution\ny = 100  # Height of the image after resizing\nz = 100  # Width of the image after resizing\n\n# Set paths for the training and validation datasets\ntrain_path = '/datasets/Project_data/train'  # Path to the training dataset\nval_path = '/datasets/Project_data/val'  # Path to the validation dataset\n\n# Get the total number of training and validation sequences\nnum_train_sequences = len(train_doc)  # Total number of training sequences\nnum_val_sequences = len(val_doc)  # Total number of validation sequences\n\n# Calculate the number of steps per epoch for training (how many batches are processed per epoch)\nif (num_train_sequences % batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences / batch_size)  # If evenly divisible, divide total sequences by batch size\nelse:\n    steps_per_epoch = (num_train_sequences // batch_size) + 1  # If not evenly divisible, round up to the next integer\n\n# Calculate the number of validation steps (how many validation batches are processed per epoch)\nif (num_val_sequences % batch_size) == 0:\n    validation_steps = int(num_val_sequences / batch_size)  # If evenly divisible, divide total validation sequences by batch size\nelse:\n    validation_steps = (num_val_sequences // batch_size) + 1  # If not evenly divisible, round up to the next integer","metadata":{"id":"icfOb8DvCN57"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1. **`curr_dt_time = datetime.datetime.now()`**:\n   - This line captures the current date and time, which will be used to generate unique filenames for saving checkpoints during training.\n\n2. **`num_epochs = 20`**:\n   - Sets the number of training epochs, which defines how many times the model will iterate over the entire dataset during training.\n\n3. **`batch_size = 16`**:\n   - Defines the batch size, which determines how many samples are processed before the model's internal parameters are updated.\n\n4. **`img_idx = [...]`**:\n   - A list of indices representing the specific images that will be used from each sequence. This allows you to select a subset of frames from each video sequence to train on.\n\n5. **`x = len(img_idx)`**:\n   - Calculates the number of images (or frames) per video sequence by counting the length of `img_idx`. This is used to shape the input data for the model.\n\n6. **`y = 100, z = 100`**:\n   - These variables define the desired height and width for the input images, which are resized during preprocessing to ensure all input images have consistent dimensions.\n\n7. **`train_path = '/datasets/Project_data/train'` and `val_path = '/datasets/Project_data/val'`**:\n   - These variables specify the paths to the directories containing the training and validation datasets, respectively.\n\n8. **`num_train_sequences = len(train_doc)`** and **`num_val_sequences = len(val_doc)`**:\n   - These lines determine the total number of training and validation sequences by checking the length of `train_doc` and `val_doc`, which are assumed to be lists of video sequence filenames or metadata.\n\n9. **`steps_per_epoch` and `validation_steps`**:\n   - These variables determine how many steps (batches) the model will process per epoch for training and validation, respectively.\n   - The calculation ensures that the number of steps accounts for both complete and incomplete batches (when the number of sequences isn't perfectly divisible by the batch size).\n\n   - If the total number of sequences is divisible by the batch size, the number of steps per epoch is simply the total sequences divided by the batch size. If not, the calculation rounds up to ensure that all data is used in the epoch.","metadata":{"id":"oCmF1ZuUu5MS"}},{"cell_type":"code","source":"# Compile the cnn_gru model using the Adam optimizer\n\n# Set up the Adam optimizer with a learning rate of 0.0001\nadam = optimizers.Adam(learning_rate=0.0001)\n\n# Initialize the cnn_gru model with the input shape (x, y, z) and dropout rate of 0.25\nmodel8 = cnn_gru(x, y, z, 0.25)\n\n# Compile the model by specifying the optimizer, loss function, and metrics to monitor\n# We use 'categorical_crossentropy' as the loss function since this is a multi-class classification task\n# 'categorical_accuracy' will be tracked as a metric during training\nmodel8.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n\n# Print the model summary to view the architecture of the model\nprint(model8.summary())","metadata":{"id":"OGD46-jpCOCg","outputId":"677df51c-faf8-495c-b946-7095b0628109"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1. **`adam = optimizers.Adam(learning_rate=0.0001)`**:\n   - This line initializes the Adam optimizer with a learning rate of 0.0001. Adam is a popular optimization algorithm that adapts the learning rate during training, making it well-suited for complex models.\n\n2. **`model10 = cnn_gru(x, y, z, 0.25)`**:\n   - This initializes the model by calling the `cnn_gru` function with the input shape `(x, y, z)` (where `x` is the number of frames per sequence, `y` is the height, and `z` is the width of the images), and a dropout rate of `0.25`. The model architecture combines Convolutional Neural Networks (CNN) with GRU (Gated Recurrent Units) layers for temporal sequence processing.\n\n3. **`model10.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['categorical_accuracy'])`**:\n   - The model is compiled with:\n     - The Adam optimizer (as defined earlier).\n     - `categorical_crossentropy` as the loss function, which is appropriate for multi-class classification problems where the labels are one-hot encoded.\n     - `categorical_accuracy` as a metric to evaluate the model's accuracy during training and validation.\n\n4. **`print(model10.summary())`**:\n   - This prints a summary of the model architecture, providing details about the layers, number of parameters, and input/output shapes. This is helpful for verifying the model configuration before training.","metadata":{"id":"BQswJglVu5MU"}},{"cell_type":"code","source":"# Call the file path creating callback_directory function\n# This function sets up the directory for saving model checkpoints, logs, and other outputs\n# It ensures that we can track the training progress and resume from checkpoints if needed\ncallbacks_list = callback_directory(curr_dt_time)\n\n# Create the train_generator and val_generator which will be used in .fit_generator\n# These generators will yield batches of images and their corresponding labels for training and validation\n# The train_generator includes data augmentation to enhance the training process\n\n# Normalization function to scale pixel values to the range [0, 1]\nnormalise = lambda a: a/255.\n\n# Create the training data generator\n# This generator reads images and labels from the training dataset located at 'train_path' using 'train_doc'\n# It applies data augmentation (True) to the images and normalizes the pixel values\ntrain_generator = generator_with_aug(train_path, train_doc, batch_size, y, z, normalise, augment=True)\n\n# Create the validation data generator\n# This generator reads images and labels from the validation dataset located at 'val_path' using 'val_doc'\n# It normalizes the pixel values but does not apply augmentation since we do not want to augment validation data\nval_generator = generator_with_aug(val_path, val_doc, batch_size, y, z, normalise, augment=False)","metadata":{"id":"ylrURe7WCOMp"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1. **`callbacks_list = callback_directory(curr_dt_time)`**:\n   - This line calls the `callback_directory` function with the current datetime (`curr_dt_time`) to create directories for saving checkpoints, logs, and other related files during training. This ensures that you can track progress, log metrics, and resume training from a checkpoint if needed.\n\n2. **`normalise = lambda a: a/255.`**:\n   - This line defines a lambda function to normalize image pixel values. The pixel values of images typically range from 0 to 255. The lambda function divides each pixel value by 255, scaling it to a range of [0, 1], which is often done to improve model performance and stability.\n\n3. **`train_generator = generator_with_aug(...)`**:\n   - This creates a training data generator (`train_generator`) using the `generator_with_aug` function.\n   - It reads the data from `train_path`, which is the path to the training data, and `train_doc`, which contains the file names and corresponding labels.\n   - The generator will yield batches of images and labels for training. Data augmentation is applied (`augment=True`) to artificially expand the training dataset and make the model more robust.\n\n4. **`val_generator = generator_with_aug(...)`**:\n   - This creates a validation data generator (`val_generator`) using the same `generator_with_aug` function.\n   - It reads the data from `val_path` (the validation data path) and `val_doc` (the corresponding labels).\n   - The generator yields batches of validation images and labels, but without any data augmentation (`augment=False`) since we want the validation set to represent real data distribution. The images are only normalized (scaled to [0, 1]).","metadata":{"id":"JaB5ZXTIu5MU"}},{"cell_type":"markdown","source":"**Next Stage - Train the model using the training and validation generators with specified parameters and callbacks**\n\n1. **`train_generator`**: This is the data generator that produces batches of images and labels for training. It is passed as the first argument to `model10.fit()`. The generator will yield data during training, with the model using this data to learn.\n\n2. **`steps_per_epoch=steps_per_epoch`**: This parameter defines how many steps (batches) are processed in each epoch. It is calculated based on the size of the training dataset and the batch size. After completing all the steps in an epoch, the model will evaluate the performance on the validation set.\n\n3. **`epochs=num_epochs`**: This specifies the total number of times the entire training dataset will be used to train the model. In this case, the model will be trained for `num_epochs` epochs.\n\n4. **`verbose=1`**: This argument controls the verbosity level. `1` means progress will be shown with a progress bar during training, including metrics for each epoch. You can set it to `0` for no output, or `2` for one line per epoch.\n\n5. **`callbacks=callbacks_list`**: This is a list of callbacks to be applied during training, such as saving model checkpoints, learning rate adjustments, or early stopping. These callbacks allow for more advanced handling of the training process.\n\n6. **`validation_data=val_generator`**: This argument provides the validation data for the model. The validation data is used to evaluate the model's performance at the end of each epoch. `val_generator` is a generator that yields validation data.\n\n7. **`validation_steps=validation_steps`**: This defines how many steps (batches) the model should process from the validation data before performing the evaluation. It ensures that the model evaluates the correct number of validation batches after each epoch.\n\n8. **`class_weight=None`**: If you have imbalanced classes in your dataset, you can use this parameter to assign different weights to different classes during training. In this case, `None` means no class weighting is applied.\n\n9. **`workers=1`**: This parameter controls the number of parallel workers used for data loading. Setting it to `1` means a single thread will handle data loading. More workers can be used to speed up data loading for large datasets.\n\n10. **`initial_epoch=0`**: This argument allows you to specify the starting epoch when resuming training. If you are continuing from a saved checkpoint, you would use the epoch number where the training was stopped. Here it is set to `0`, indicating that training will start from the beginning.","metadata":{"id":"xCulhdliu5MU"}},{"cell_type":"code","source":"# Fit the model\n# This line trains the model using the training data generated by 'train_generator' and evaluates it using the validation data from 'val_generator'.\n# The model will run for a specified number of epochs, and after each epoch, the performance will be evaluated on the validation dataset.\n\nhistory_model8 = model8.fit(\n    train_generator,                         # The generator that yields training batches of data (images and labels)\n    steps_per_epoch=steps_per_epoch,         # The number of steps (batches) per epoch, calculated based on the training dataset size and batch size\n    epochs=num_epochs,                       # The total number of epochs (iterations over the entire dataset) for training\n    verbose=1,                               # Display progress during training (1 means displaying a progress bar and metrics)\n    callbacks=callbacks_list,                # List of callbacks to be applied during training (e.g., saving checkpoints, early stopping, etc.)\n    validation_data=val_generator,           # The validation data generator that yields validation batches of data\n    validation_steps=validation_steps,       # The number of steps (batches) to run on the validation data per epoch\n    class_weight=None,                       # Class weights for handling imbalanced datasets (None means no class weighting is applied)\n    workers=1,                               # The number of workers for loading the data in parallel (1 means single-threaded data loading)\n    initial_epoch=0                          # The starting epoch, useful for resuming training from a specific epoch\n)","metadata":{"id":"mndixNtCCOVB","outputId":"41e6e6c6-9ad7-4401-9fc3-5a3b79523458"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print_model_metrics(\"Model No. 8 - Transfer Learrning using Mobilenet and GRU Model\", history_model8)","metadata":{"id":"_L4S-h6Bu5MV","outputId":"0a7953ca-fb9e-411e-b2c2-dc46ee58e23f"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualizing Model No. 8\n- Plots training and validation accuracy/loss to visualize model performance.\n- Helps assess learning and generalization to new data.\n- Identifies overfitting or underfitting issues.\n- Aids in tuning hyperparameters for better performance.\n- Offers insights to optimize learning and adjust the model.","metadata":{"id":"uKRX7nURu5MV"}},{"cell_type":"code","source":"plot_history(history_model8, num_epochs)","metadata":{"id":"K_CeQTanCbLb","outputId":"982ba3dd-ccec-448e-af82-1f3a7b1fe39a"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model No. 8 - Observations\n\n- **Training Accuracy**: Increased steadily, reaching 99.92% by epoch 30.\n- **Validation Accuracy**: Fluctuated, peaking at 95.00% by epoch 16, then hovering around 92.00% toward the final epochs.\n- **Loss**: Training loss consistently decreased, with validation loss showing steady improvement, especially after epoch 15.\n- **Learning Rate**: Initially set at 1e-4, remained constant throughout the epochs without further reduction, indicating steady learning.\n- **Model Saving**: The model was saved after each epoch, with notable progress, especially in the reduction of validation loss.\n\n**Overall Summary**\n\nAcross the 30 epochs, the model demonstrated continual improvements in both training and validation accuracy. Training accuracy progressed steadily, reaching an impressive 100% by epoch 30, while validation accuracy fluctuated, peaking at 95% before stabilizing at 92% by the end of the training. Training loss steadily decreased, and validation loss followed a similar pattern, especially after epoch 15, with the most notable improvement seen after epoch 28. The learning rate remained constant at 1e-4 throughout the training, without further adjustments, ensuring the model continued refining its predictions. Model checkpoints were saved after each epoch, capturing incremental improvements in performance. Despite fluctuations in validation accuracy, the model showed strong generalization and consistent progress.","metadata":{"id":"vHP5kw8J6vJ4"}},{"cell_type":"markdown","source":"# ConvLSTM Model for Sequence-based Image Processing\n\nThe **ConvLSTM model** is designed for sequence-based image processing, particularly for tasks involving video or time-series image data. It combines convolutional layers (Conv2D) and Long Short-Term Memory (LSTM) units within a single architecture. The **Conv2D** layers extract spatial features from each frame, while the **ConvLSTM2D** layer captures both spatial and temporal dependencies across frames. The **TimeDistributed** wrapper applies layers like Conv2D and Dense across each time step independently, making it suitable for video classification, action recognition, or next-frame prediction. The model is effective for learning from sequences of images, handling both spatial and temporal information.","metadata":{"id":"E8UnEa7Fybn-"}},{"cell_type":"markdown","source":"## Creating ConvLSTM Model with Data Augmentation\n\nThis model is called a **ConvLSTM model** (Convolutional Long Short-Term Memory). It combines convolutional layers and LSTM layers to process sequences of images or video frames. Here's a breakdown of what the model is doing:\n\n1. **TimeDistributed Wrapper**: This wrapper allows the model to apply a 2D convolution (`Conv2D`) across each frame in the sequence independently. This is crucial for processing sequences of images, such as video frames.\n\n2. **Conv2D Layers**: These layers apply 2D convolutions to the individual frames of the sequence, helping the model capture spatial features in each frame.\n\n3. **ConvLSTM2D Layer**: The key feature of this model is the ConvLSTM2D layer. It is a combination of convolutional operations with LSTM units, which allows the model to capture both spatial (from Conv2D) and temporal (from LSTM) dependencies across frames in a video or sequence of images.\n\n4. **BatchNormalization**: This layer normalizes the activations to stabilize training, helping the model learn faster and potentially generalize better.\n\n5. **TimeDistributed Dense Layer**: This layer applies a fully connected (dense) layer to each time step (frame) in the sequence, treating each frame independently.\n\n6. **GlobalAveragePooling2D**: This layer reduces the spatial dimensions (height and width) by computing the average of the entire spatial feature map, helping reduce the number of parameters and avoiding overfitting.\n\n7. **Dense Layer**: The final fully connected layer performs classification with 5 output classes, using softmax activation for multi-class classification.","metadata":{"id":"uU6Npqycybn-"}},{"cell_type":"code","source":"def convLSTM_model(x, y, z):\n    \"\"\"\n    Build a model using Conv2D, ConvLSTM2D, and TimeDistributed layers for sequence-based image processing.\n\n    Parameters:\n    - x: int, number of frames in the sequence\n    - y: int, height of the input image\n    - z: int, width of the input image\n\n    Returns:\n    - model: Keras Sequential model\n    \"\"\"\n    # Initialize the model\n    model = Sequential()\n\n    # Add a TimeDistributed wrapper with Conv2D for 2D convolution over each frame of the sequence\n    model.add(TimeDistributed(\n        Conv2D(8, (3, 3), activation='relu'), input_shape=(x, y, z, 3)))  # 3 channels (RGB)\n    model.add(BatchNormalization())  # Normalize the output to stabilize training\n\n    # Add another TimeDistributed Conv2D layer with 16 filters\n    model.add(TimeDistributed(\n        Conv2D(16, (3, 3), activation='relu')))\n    model.add(BatchNormalization())  # Apply BatchNormalization\n\n    # Add ConvLSTM2D layer to capture temporal dependencies across frames\n    model.add(ConvLSTM2D(8, kernel_size=(3, 3), return_sequences=False))\n    model.add(BatchNormalization())  # Normalize the output of ConvLSTM2D\n\n    # Add TimeDistributed Dense layer to apply a fully connected layer across all time steps\n    model.add(TimeDistributed(\n        Dense(64, activation='relu')))\n    model.add(BatchNormalization())  # Normalize output\n\n    # Apply Global Average Pooling to reduce spatial dimensions (height and width)\n    model.add(GlobalAveragePooling2D())\n\n    # Add a fully connected Dense layer with 64 units for further processing\n    model.add(Dense(64, activation='relu'))\n\n    # Output layer for classification with 5 classes, using softmax for multi-class classification\n    model.add(Dense(5, activation='softmax'))\n\n    return model","metadata":{"id":"ZvpKlol3ybn-"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Evaluating Model No. 9\n- **Model**: Model 9 - Transfer Learrning using Mobilenet and GRU\n- **Batch Size**: 32\n- **Number of Epochs**: 30\n- **Image Resolution**: 100x100\n- **Optimizer**: Adam\n- **Normalization Method**: Pixel values divided by 255 (scaling between 0 and 1)","metadata":{"id":"kT3DWG99ybn-"}},{"cell_type":"code","source":"# Set the current date and time to create unique filenames for checkpoints\ncurr_dt_time = datetime.datetime.now()\n\n# Define the number of epochs for training\nnum_epochs = 30  # Set the number of epochs to train the model\n\n# Define batch size\nbatch_size = 32  # Set the batch size to 16\n\n# Define the list of indices for the images in the sequence to be used for each video frame\nimg_idx = [0, 1, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 27, 28, 29]\nx = len(img_idx)  # The number of images to use per video sequence\n\n# Define image resolution\ny = 100  # Height of the image after resizing\nz = 100  # Width of the image after resizing\n\n# Set paths for the training and validation datasets\ntrain_path = '/datasets/Project_data/train'  # Path to the training dataset\nval_path = '/datasets/Project_data/val'  # Path to the validation dataset\n\n# Get the total number of training and validation sequences\nnum_train_sequences = len(train_doc)  # Total number of training sequences\nnum_val_sequences = len(val_doc)  # Total number of validation sequences\n\n# Calculate the number of steps per epoch for training (how many batches are processed per epoch)\nif (num_train_sequences % batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences / batch_size)  # If evenly divisible, divide total sequences by batch size\nelse:\n    steps_per_epoch = (num_train_sequences // batch_size) + 1  # If not evenly divisible, round up to the next integer\n\n# Calculate the number of validation steps (how many validation batches are processed per epoch)\nif (num_val_sequences % batch_size) == 0:\n    validation_steps = int(num_val_sequences / batch_size)  # If evenly divisible, divide total validation sequences by batch size\nelse:\n    validation_steps = (num_val_sequences // batch_size) + 1  # If not evenly divisible, round up to the next integer","metadata":{"id":"m1virfFZybn-"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras import optimizers\n\n# Define the Adam optimizer with a learning rate of 0.01\noptimizer = optimizers.Adam(learning_rate=0.01)\n\nmodel9 = convLSTM_model(x, y, z)\n\n# Compile the model with categorical crossentropy loss and categorical accuracy metric\nmodel9.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n\n# Print the model summary\nprint(model9.summary())","metadata":{"id":"sKpCrkx3ybn-","outputId":"2cd6f325-d751-40bc-e4d6-aed1b3266298"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Call the file path creating callback_directory function\n# This function sets up the directory for saving model checkpoints, logs, and other outputs\n# It ensures that we can track the training progress and resume from checkpoints if needed\ncallbacks_list = callback_directory(curr_dt_time)\n\n# Create the train_generator and val_generator which will be used in .fit_generator\n# These generators will yield batches of images and their corresponding labels for training and validation\n# The train_generator includes data augmentation to enhance the training process\n\n# Normalization function to scale pixel values to the range [0, 1]\nnormalise = lambda a: a/255.\n\n# Create the training data generator\n# This generator reads images and labels from the training dataset located at 'train_path' using 'train_doc'\n# It applies data augmentation (True) to the images and normalizes the pixel values\ntrain_generator = generator_with_aug(train_path, train_doc, batch_size, y, z, normalise, augment=True)\n\n# Create the validation data generator\n# This generator reads images and labels from the validation dataset located at 'val_path' using 'val_doc'\n# It normalizes the pixel values but does not apply augmentation since we do not want to augment validation data\nval_generator = generator_with_aug(val_path, val_doc, batch_size, y, z, normalise, augment=False)","metadata":{"id":"CzgXg5coybn_"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fit the model\n# This line trains the model using the training data generated by 'train_generator' and evaluates it using the validation data from 'val_generator'.\n# The model will run for a specified number of epochs, and after each epoch, the performance will be evaluated on the validation dataset.\n\nhistory_model9 = model9.fit(\n    train_generator,                         # The generator that yields training batches of data (images and labels)\n    steps_per_epoch=steps_per_epoch,         # The number of steps (batches) per epoch, calculated based on the training dataset size and batch size\n    epochs=num_epochs,                       # The total number of epochs (iterations over the entire dataset) for training\n    verbose=1,                               # Display progress during training (1 means displaying a progress bar and metrics)\n    callbacks=callbacks_list,                # List of callbacks to be applied during training (e.g., saving checkpoints, early stopping, etc.)\n    validation_data=val_generator,           # The validation data generator that yields validation batches of data\n    validation_steps=validation_steps,       # The number of steps (batches) to run on the validation data per epoch\n    class_weight=None,                       # Class weights for handling imbalanced datasets (None means no class weighting is applied)\n    workers=1,                               # The number of workers for loading the data in parallel (1 means single-threaded data loading)\n    initial_epoch=0                          # The starting epoch, useful for resuming training from a specific epoch\n)","metadata":{"id":"OtWGXJ5bybn_","outputId":"90ab7977-1f75-4f86-af5f-e27af9859297"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print_model_metrics(\"Model No. 9 - ConvLSTM Model for Sequence-based Image Processing\", history_model9)","metadata":{"id":"B-biJY6Gybn_","outputId":"38cada0d-a23a-4351-918b-e838e2a54dd5"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualizing Model No. 9\nPlotting training and validation accuracy/loss is an essential tool for visualizing a model's performance during the training process. By graphing both the training and validation curves, it becomes easier to assess how well the model is learning from the data and generalizing to new, unseen data. These plots help identify potential issues such as **overfitting**, where the model performs well on training data but poorly on validation data, or **underfitting**, where the model struggles to learn from both training and validation data.\n\nThis visualization also provides insights into the model’s convergence, indicating whether it has learned the data distribution effectively. Moreover, monitoring these plots aids in **tuning hyperparameters** like learning rate, batch size, or network architecture to improve model accuracy and generalization. Overall, accuracy/loss plots are valuable for optimizing the learning process, adjusting the model's complexity, and ensuring it performs effectively on new data.","metadata":{"id":"8D0LLgg_ybn_"}},{"cell_type":"code","source":"plot_history(history_model9, num_epochs)","metadata":{"id":"FinIASZyyboA","outputId":"6af62ab5-0571-4b1e-9efe-05f867385f19"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model No. 9 - Observations\n\n- **Training Loss**: Decreases steadily from 1.3823 (Epoch 1) to 0.6592 (Epoch 30), showing consistent improvement.\n- **Training Accuracy**: Starts at 38.16% (Epoch 1) and increases to 73.15% (Epoch 30), indicating positive training progress.\n- **Validation Loss**: Fluctuates but shows improvement, starting at 1.6523 (Epoch 1) and ending at 0.6657 (Epoch 30), with significant reductions.\n- **Validation Accuracy**: Starts at 22% (Epoch 1) and improves to 77% (Epoch 30), demonstrating considerable progress in validation performance.\n- **Learning Rate**: Reduced multiple times via `ReduceLROnPlateau`, from 0.01 to 3.9062e-05, contributing to training stability.\n- **Model Saving**: Model checkpoints were saved after each epoch with corresponding loss and accuracy values.\n\n### Overall Summary\n\nThe model shows consistent improvement in both training and validation metrics. Training accuracy rises from 38.16% to 73.15%, and validation accuracy improves from 22% to 77%, indicating that the model is learning and generalizing well. The steady decrease in both training and validation loss suggests the model is effectively fitting the data. The learning rate reduction helped stabilize training and achieve these improvements. The model's performance suggests it is well-optimized for the task.","metadata":{"id":"dmPE7TQ_yboA"}},{"cell_type":"markdown","source":"# TimeDistributed CNN with Dense and Global Average Pooling\n\nA **TimeDistributed Convolutional Neural Network (CNN)** is designed for sequence-based image data, such as videos or image sequences. It applies convolutional operations independently to each frame in the sequence using the `TimeDistributed` wrapper. This allows the model to extract spatial features from each frame while maintaining temporal dependencies across frames. After feature extraction, **Global Average Pooling** reduces the spatial dimensions, summarizing the feature maps into a single vector. The model then processes this vector through fully connected (`Dense`) layers, allowing it to make predictions, making it ideal for sequence classification tasks like action recognition or gesture classification.","metadata":{"id":"n6B0B6qCyboA"}},{"cell_type":"markdown","source":"## Creating TimeDistributed Conv2D with Dense and Data Augmentation\n\nThe model described above is a **TimeDistributed Convolutional Neural Network (CNN)** followed by **Global Average Pooling** and a fully connected (Dense) network. The model is designed to handle sequence-based image data (such as videos or image sequences) using **TimeDistributed** wrappers to apply the same CNN layers across each time step in the sequence.\n\n**Key Components of the Model**\n\n1. **TimeDistributed Layer:**\n   - The `TimeDistributed` wrapper allows the model to apply a layer (like `Conv2D`, `MaxPooling2D`) to each frame of the sequence independently. This means the same operations are applied to each time step/frame in the sequence, ensuring the model can process temporal data (sequence of images or frames from a video).\n\n2. **Convolutional Layers (Conv2D):**\n   - The model uses several `Conv2D` layers with increasing numbers of filters (32, 64, 128). These layers are responsible for extracting spatial features (such as edges, textures, patterns) from each individual frame in the sequence.\n   - The kernel size is `(3, 3)`, meaning that the convolution operation considers a 3x3 neighborhood of each pixel in the input image.\n\n3. **MaxPooling Layers (MaxPooling2D):**\n   - After each `Conv2D` layer, the model applies a `MaxPooling2D` layer with a pool size of `(2, 2)`, which helps reduce the spatial dimensions (height and width) of the feature maps while retaining the most significant information. This reduces the computational cost and helps avoid overfitting.\n\n4. **Batch Normalization:**\n   - After each convolution and pooling operation, a `BatchNormalization` layer is applied. Batch normalization normalizes the activations of each layer, making training more stable and faster by reducing internal covariate shift. It helps prevent overfitting by regularizing the model and smoothing the optimization process.\n\n5. **Global Average Pooling (GlobalAveragePooling3D):**\n   - After the convolutional and pooling layers, the model applies `GlobalAveragePooling3D` to reduce the output from 3D (height, width, depth) to 1D, effectively averaging the feature maps across all spatial dimensions. This operation is used to create a fixed-size output regardless of the input size.\n\n6. **Fully Connected Layers (Dense):**\n   - After pooling, the model uses a dense layer with 256 units to further process the features.\n   - The final output layer is a `Dense` layer with 5 units (for 5 classes) and a `softmax` activation function, which is commonly used for multi-class classification tasks. It outputs a probability distribution over the 5 possible classes.\n\n**What the Model Does**\n\nThis model is well-suited for sequence-based image processing tasks, such as **video classification**, where each frame in the sequence needs to be processed individually (for spatial features) but the temporal dependencies between frames need to be captured. The `TimeDistributed` wrapper ensures that the same convolution operations are applied independently to each frame in the sequence. The `GlobalAveragePooling3D` reduces the dimensions of the feature maps, followed by dense layers for final decision-making.","metadata":{"id":"29V6v7f1yboA"}},{"cell_type":"code","source":"def timeDistributedConv2D(x, y, z):\n    \"\"\"\n    Build a model using Conv2D layers, MaxPooling2D, and TimeDistributed layers for sequence-based image processing.\n\n    Parameters:\n    - x: int, number of frames in the sequence\n    - y: int, height of the input image\n    - z: int, width of the input image\n\n    Returns:\n    - model: Keras Sequential model\n    \"\"\"\n    # Initialize the model\n    model = Sequential()\n\n    # Add a TimeDistributed wrapper with Conv2D for 2D convolution over each frame of the sequence\n    model.add(TimeDistributed(\n        Conv2D(32, (3, 3), activation='relu'), input_shape=(x, y, z, 3)))  # 3 channels (RGB)\n    model.add(TimeDistributed(MaxPooling2D((2, 2))))  # Apply MaxPooling2D layer\n    model.add(BatchNormalization())  # Normalize the output to stabilize training\n\n    # Add another TimeDistributed Conv2D layer with 64 filters\n    model.add(TimeDistributed(\n        Conv2D(64, (3, 3), activation='relu')))\n    model.add(TimeDistributed(MaxPooling2D((2, 2))))  # Apply MaxPooling2D layer\n    model.add(BatchNormalization())  # Apply BatchNormalization\n\n    # Add another TimeDistributed Conv2D layer with 128 filters\n    model.add(TimeDistributed(\n        Conv2D(128, (3, 3), activation='relu')))\n    model.add(TimeDistributed(MaxPooling2D((2, 2))))  # Apply MaxPooling2D layer\n    model.add(BatchNormalization())  # Apply BatchNormalization\n\n    # Apply Global Average Pooling to reduce the spatial dimensions\n    model.add(GlobalAveragePooling3D())\n\n    # Add a fully connected Dense layer with 256 units for further processing\n    model.add(Dense(256, activation='relu'))\n    model.add(BatchNormalization())  # Apply BatchNormalization\n\n    # Output layer for classification with 5 classes, using softmax for multi-class classification\n    model.add(Dense(5, activation='softmax'))\n\n    return model","metadata":{"id":"JNeQ1VPsyboA"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Evaluating Model No. 10\n- **Model**: Model 10 - TimeDistributed Conv2D with Dense and Data Augmentation\n- **Batch Size**: 32\n- **Number of Epochs**: 30\n- **Image Resolution**: 100x100\n- **Optimizer**: Adam\n- **Normalization Method**: Pixel values divided by 255 (scaling between 0 and 1)","metadata":{"id":"8V5MskaiyboA"}},{"cell_type":"code","source":"# Set the current date and time to create unique filenames for checkpoints\ncurr_dt_time = datetime.datetime.now()\n\n# Define the number of epochs for training\nnum_epochs = 30  # Set the number of epochs to train the model\n\n# Define batch size\nbatch_size = 32  # Set the batch size to 16\n\n# Define the list of indices for the images in the sequence to be used for each video frame\nimg_idx = [0, 1, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 27, 28, 29]\nx = len(img_idx)  # The number of images to use per video sequence\n\n# Define image resolution\ny = 100  # Height of the image after resizing\nz = 100  # Width of the image after resizing\n\n# Set paths for the training and validation datasets\ntrain_path = '/datasets/Project_data/train'  # Path to the training dataset\nval_path = '/datasets/Project_data/val'  # Path to the validation dataset\n\n# Get the total number of training and validation sequences\nnum_train_sequences = len(train_doc)  # Total number of training sequences\nnum_val_sequences = len(val_doc)  # Total number of validation sequences\n\n# Calculate the number of steps per epoch for training (how many batches are processed per epoch)\nif (num_train_sequences % batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences / batch_size)  # If evenly divisible, divide total sequences by batch size\nelse:\n    steps_per_epoch = (num_train_sequences // batch_size) + 1  # If not evenly divisible, round up to the next integer\n\n# Calculate the number of validation steps (how many validation batches are processed per epoch)\nif (num_val_sequences % batch_size) == 0:\n    validation_steps = int(num_val_sequences / batch_size)  # If evenly divisible, divide total validation sequences by batch size\nelse:\n    validation_steps = (num_val_sequences // batch_size) + 1  # If not evenly divisible, round up to the next integer","metadata":{"id":"K50WrCSJyboA"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the Adam optimizer with a learning rate of 0.01\nadam = optimizers.Adam(learning_rate=0.01)\n\nmodel10 = timeDistributedConv2D(x, y, z)\n\n# Compile the model with categorical crossentropy loss and categorical accuracy metric\nmodel10.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n\n# Print the model summary\nprint(model10.summary())","metadata":{"id":"hWME6KYcyboB","outputId":"27118ace-09db-4c0f-9592-ff937e8d2432"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Call the file path creating callback_directory function\n# This function sets up the directory for saving model checkpoints, logs, and other outputs\n# It ensures that we can track the training progress and resume from checkpoints if needed\ncallbacks_list = callback_directory(curr_dt_time)\n\n# Create the train_generator and val_generator which will be used in .fit_generator\n# These generators will yield batches of images and their corresponding labels for training and validation\n# The train_generator includes data augmentation to enhance the training process\n\n# Normalization function to scale pixel values to the range [0, 1]\nnormalise = lambda a: a/255.\n\n# Create the training data generator\n# This generator reads images and labels from the training dataset located at 'train_path' using 'train_doc'\n# It applies data augmentation (True) to the images and normalizes the pixel values\ntrain_generator = generator_with_aug(train_path, train_doc, batch_size, y, z, normalise, augment=True)\n\n# Create the validation data generator\n# This generator reads images and labels from the validation dataset located at 'val_path' using 'val_doc'\n# It normalizes the pixel values but does not apply augmentation since we do not want to augment validation data\nval_generator = generator_with_aug(val_path, val_doc, batch_size, y, z, normalise, augment=False)","metadata":{"id":"OBPHHxVmyboB"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fit the model\n# This line trains the model using the training data generated by 'train_generator' and evaluates it using the validation data from 'val_generator'.\n# The model will run for a specified number of epochs, and after each epoch, the performance will be evaluated on the validation dataset.\n\nhistory_model10 = model10.fit(\n    train_generator,                         # The generator that yields training batches of data (images and labels)\n    steps_per_epoch=steps_per_epoch,         # The number of steps (batches) per epoch, calculated based on the training dataset size and batch size\n    epochs=num_epochs,                       # The total number of epochs (iterations over the entire dataset) for training\n    verbose=1,                               # Display progress during training (1 means displaying a progress bar and metrics)\n    callbacks=callbacks_list,                # List of callbacks to be applied during training (e.g., saving checkpoints, early stopping, etc.)\n    validation_data=val_generator,           # The validation data generator that yields validation batches of data\n    validation_steps=validation_steps,       # The number of steps (batches) to run on the validation data per epoch\n    class_weight=None,                       # Class weights for handling imbalanced datasets (None means no class weighting is applied)\n    workers=1,                               # The number of workers for loading the data in parallel (1 means single-threaded data loading)\n    initial_epoch=0                          # The starting epoch, useful for resuming training from a specific epoch\n)","metadata":{"id":"1k6EbUYWyboB","outputId":"7182c08d-a60c-48a3-b359-be65bf8c23b1"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print_model_metrics(\"Model No. 10 - TimeDistributed Conv2D with Dense and Data Augmentation\", history_model10)","metadata":{"id":"mlRUhrpsyboB","outputId":"0b7a14bb-53bd-4598-fa3e-7bdb5cba2c92"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualizing Model No. 10\n\nPlotting training and validation accuracy/loss is crucial for evaluating a model's performance. By visualizing both curves, it’s easier to assess how well the model learns and generalizes to unseen data. These plots help detect **overfitting**, where the model excels on training data but underperforms on validation data, and **underfitting**, where it fails to learn from both. Additionally, they provide insights into model convergence, indicating whether it effectively learns the data distribution. Monitoring these plots helps in **tuning hyperparameters** like learning rate and batch size to optimize the model’s performance and generalization.","metadata":{"id":"bUX7QvuEyboB"}},{"cell_type":"code","source":"plot_history(history_model10, num_epochs)","metadata":{"id":"XhgRYeeOyboB","outputId":"0d210bd8-63cc-4c62-fc69-cd226e9266c9"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model No. 10 - Observations\n\n- **Training Accuracy**: Increased consistently, peaking at 90.12% by epoch 30.\n- **Validation Accuracy**: Fluctuated, reaching a peak of 71.00% by epoch 29, and settling at 68.00% by epoch 30.\n- **Loss**: Training loss steadily decreased, with validation loss showing a general downward trend, especially after epoch 19, with the lowest value of 0.62413 at epoch 29.\n- **Learning Rate**: Initially set at 0.01, then reduced multiple times using the ReduceLROnPlateau scheduler, reaching a final value of 1.5625e-4 by epoch 30.\n- **Model Saving**: The model was saved after each epoch, particularly after epochs with improvements in validation loss and accuracy, with significant progress made after epochs 20, 22, 23, 25, 26, 27, and 29.\n\n**Overall Summary**\n\nThe model showed steady improvement in both training and validation accuracy across 30 epochs. Training accuracy improved from 41.55% at epoch 1 to 90.12% by epoch 30. Validation accuracy showed fluctuations, reaching its highest point of 71.00% at epoch 29 and stabilizing at 68.00% by the final epoch. Training loss decreased consistently, and the validation loss followed a similar trend, with its lowest value of 0.62413 at epoch 29. The learning rate was initially set at 0.01 and gradually reduced using the ReduceLROnPlateau scheduler, reaching a final value of 1.5625e-4 by epoch 30. Model checkpoints were saved after each epoch, capturing key improvements, especially in validation loss and accuracy. Despite the fluctuations in validation performance, the overall trend indicates that the model was able to learn and improve its generalization.","metadata":{"id":"JedAPVaYyboB"}},{"cell_type":"markdown","source":"# Hand Gesture Recognition Project Summary\n\nThe objective of this project was to develop a gesture recognition system to control a smart TV using a webcam. This would enable users to interact without the need for a remote control. After evaluating various models, **Model 8** (Transfer Learning with MobileNet and GRU) emerged as the top performer, achieving an impressive **99.92% training accuracy** and **90% validation accuracy**, demonstrating excellent generalization across unseen data. This model effectively recognized target gestures, ensuring seamless and precise interaction for TV control.\n\n**Model 4** (CNN-GRU with Transfer Learning) also performed well, with **95.78% training accuracy** and **80% validation accuracy**. It showed strong capabilities in processing gestures and responding to control commands, despite a slight gap between training and validation performance.\n\n**Model 9** (TimeDistributed ConvLSTM Model) displayed consistent improvement, with **73.15% training accuracy** and **77% validation accuracy**. Although it performed lower than the top models, it showed reliable learning and generalization for continuous gesture data processing.\n\nThese models successfully recognized gestures such as **Thumbs Up/Down**, **Left/Right Swipe**, and **Stop**, supporting the goal of creating a hands-free, intuitive TV control system. **Model 8** is recommended as the best option, followed by **Model 4** and **Model 9**, based on their strong performance and effective generalization.\n\nThis gesture-based control system will redefine the smart TV experience, providing users with a seamless and convenient interaction method.\n\n---\n\n## Model Evaluation Details\n\nHere’s a concise **evaluation summary** in a table format:\n\n| **Model Number** | **Model**                                  | **Evaluation Details**                                               | **Reason (Why we chose this model)**                                     |\n|------------------|--------------------------------------------|-----------------------------------------------------------------------|-------------------------------------------------------------------------|\n| 1                | Conv3D Architecture                        | 20 Epochs, Batch Size: 32, Image Resolution: 100x100                   | Provides basic 3D convolution, suitable for video processing but shows room for improvement. |\n| 2                | Conv3D Architecture                        | 30 Epochs, Batch Size: 32, Image Resolution: 100x100                   | Longer training, better refinement, but still struggles with high validation fluctuations. |\n| 3                | CNN-LSTM Model                             | 30 Epochs, Batch Size: 32, Image Resolution: 100x100                   | Combines CNN with LSTM for sequential learning, but SGD optimization limits convergence. |\n| 4                | CNN-GRU with Transfer Learning             | 20 Epochs, Batch Size: 32, Image Resolution: 100x100                   | Transfer learning improves generalization; fast convergence with Adam optimizer. |\n| 5                | CNN, LSTM, and GRU with Transfer Learning  | 20 Epochs, Batch Size: 32, Image Resolution: 100x100                   | Advanced architecture improves performance, but may not scale as effectively with more data. |\n| 6                | Conv3D with Data Augmentation              | 20 Epochs, Batch Size: 20, Image Resolution: 100x100                   | Data augmentation increases robustness but slightly lower batch size affects training time. |\n| 7                | CNN, LSTM, and GRU with Transfer Learning  | 30 Epochs, Batch Size: 32, Image Resolution: 100x100                   | Transfer learning with combined models enhances feature extraction for gesture recognition. |\n| 8                | Transfer Learning with MobileNet and GRU   | 30 Epochs, Batch Size: 16, Image Resolution: 100x100                   | Lightweight MobileNet with GRU offers efficient performance, but smaller batch size. |\n| 9                | TimeDistributed ConvLSTM Model             | 30 Epochs, Batch Size: 32, Image Resolution: 100x100                   | Efficient architecture; balanced batch size and optimizer enhance performance. |\n| 10               | TimeDistributed Conv2D with Dense         | 30 Epochs, Batch Size: 32, Image Resolution: 100x100                   | Good for frame-level classification, but may not capture sequential dependencies effectively. |\n\n---\n\n## Top Models for Evaluation Purpose\n\nThe top models selected for evaluation are:\n\n1. **Model 4** (CNN-GRU with Transfer Learning)\n   - **Reason**: This model stands out due to its use of transfer learning, which enhances generalization and accelerates convergence. It performs well in terms of training time and accuracy, especially with the Adam optimizer. This makes it ideal for quick adaptation to new tasks and provides a solid foundation for gesture recognition.\n\n2. **Model 7** (CNN, LSTM, and GRU with Transfer Learning)\n   - **Reason**: Combining CNN, LSTM, and GRU with transfer learning improves feature extraction, making it effective for complex gesture recognition tasks. Its ability to learn temporal dependencies is a key advantage, making it suitable for dynamic input sequences and real-time gesture recognition.\n\n3. **Model 9** (TimeDistributed ConvLSTM Model)\n   - **Reason**: This model offers an efficient architecture by combining convolutional layers with LSTM, capturing both spatial and temporal patterns effectively. It strikes a good balance between performance and efficiency, with a suitable batch size and optimizer, making it a strong choice for video or gesture sequence recognition.\n\n---\n\n## Model Summary Details\n\nHere’s a summary table for the model performance:\n\n| **Model No.** | **Model**                             | **Result**                       | **Description**                                                                |\n|---------------|---------------------------------------|----------------------------------|--------------------------------------------------------------------------------|\n| 1             | Conv3D Architecture                   | Training: 61.09%, Validation: 32% | Steady training improvement but stagnant validation accuracy, suggesting potential overfitting. |\n| 2             | Conv3D Architecture                   | Training: 62.59%, Validation: 56% | Continuous improvement in both training and validation, with steady progress despite some fluctuation. |\n| 3             | CNN-LSTM Model                        | Training: 41.78%, Validation: 34% | Steady progress in training, but validation accuracy lags, indicating a gap likely due to overfitting. |\n| 4             | CNN-GRU with Transfer Learning        | Training: 95.78%, Validation: 80% | Significant improvement in both training and validation accuracy, with some gap still remaining. |\n| 5             | CNN, LSTM, and GRU with Transfer Learning | Training: 98.49%, Validation: 75% | Strong training performance with consistent improvement in validation accuracy, though with some fluctuation. |\n| 6             | Conv3D with Data Augmentation         | Training: 60.86%, Validation: 59% | Notable improvement in both training and validation, with consistent performance across epochs. |\n| 7             | CNN, LSTM, and GRU with Transfer Learning | Training: 98.87%, Validation: 75% | Excellent training accuracy but some validation fluctuations, requiring further optimization. |\n| 8             | Transfer Learning with MobileNet and GRU | Training: 99.92%, Validation: 90% | Strong training performance with fluctuating but improving validation accuracy, reflecting good generalization. |\n| 9             | TimeDistributed ConvLSTM Model        | Training: 73.15%, Validation: 77% | Consistent improvement in both training and validation, suggesting effective learning and generalization. |\n| 10            | TimeDistributed Conv2D with Dense    | Training: 90.12%, Validation: 68% | Strong training progress with some fluctuation in validation accuracy, indicating solid learning and generalization. |\n\n---\n\n## Conclusion - Top 3 Models Selection\n\nBased on the performance summary, here are the top 3 models:\n\n### 1. **Model 8 - Transfer Learning with MobileNet and GRU**\n- **Training Accuracy**: 99.92%, **Validation Accuracy**: 90%\n- **Strengths**:\n  - Highest training accuracy and strong validation performance.\n  - Shows significant improvement in validation accuracy, reaching 90%.\n  - Excellent ability to generalize, making it the top choice for gesture recognition in smart TV control.\n\n### 2. **Model 4 - CNN-GRU with Transfer Learning**\n- **Training Accuracy**: 95.78%, **Validation Accuracy**: 80%\n- **Strengths**:\n  - Significant improvement in both training and validation accuracy.\n  - Effective processing of gestures for control, with strong generalization thanks to transfer learning.\n\n### 3. **Model 9 - TimeDistributed ConvLSTM Model**\n- **Training Accuracy**: 73.15%, **Validation Accuracy**: 77%\n- **Strengths**:\n  - Consistent improvement in both training and validation accuracy.\n  - Suitable for continuous processing of gesture data, demonstrating effective learning and generalization.\n\n---\n\nThese top models ensure **accurate**, **efficient**, and **seamless interaction** for gesture-based smart TV control. Model 8, in particular, leads","metadata":{"id":"4mwt_eZXyboC"}},{"cell_type":"markdown","source":"```markdown\n## Content Terms & Conditions:\n\n1. Above rates are for **one do-follow external link** per article.\n2. For **two do-follow external links**, we charge a **50% addition** to our base rates.\n3. We **do not allow/offer no-follow external links** per article.\n4. Be assured that the published post will be **permanent** and will **not** be marked as a sponsored post.\n5. **Disclaimer and Sponsorship**: We do not provide any disclaimer or sponsor labels in the content. All links are presented transparently as editorial content without any sponsor indication.\n6. **Lifetime Link Duration**: The links provided will remain **active for the lifetime of the article** unless otherwise agreed upon. We do not remove or alter the links after publication unless requested under exceptional circumstances.\n7. **Links Provided Per Article**: We provide **one do-follow external link per article** as part of our standard offering. For articles with more than one link, additional charges apply as mentioned above.\n\n## Payment Terms & Conditions:\n\n1. Payment can be made through **Paypal** for Non-Indian clients and **Bank Transfer** for Indian customers.\n2. Due date for the payment will be **5 days from the date of invoice**.\n3. Post the due date, **10% late payment fees** will be applied on the invoice during the grace period of **15 days**.\n4. Post the grace period, tasks will be considered as invalid, the company will be listed under **blacklist**, and no further orders will be taken.\n```\n\nThis version adds the necessary points about disclaimers, sponsorships, link duration, and the number/type of links in each article as requested. Let me know if you need further modifications!","metadata":{"id":"L29VJMeo_U_i"}}]}